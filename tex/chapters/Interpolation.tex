% \begin{my_pros_cons_table}{
%         \item 1
%         \item 2
%     }{
%         \item 1
%         \item 2
%     }
% \end{my_pros_cons_table}
\newcommand{\RobItPlot}{fitted to different (SCL45) NDVI {TS}. Iterations of a robustifying refit (as indicated in section~\ref{sec:loess_robustify}) are also displayed.}


\chapter{Interpolation} \label{sec:itpl}
	{% Roadmap
		The need for interpolating the NDVI {TS} was established in the precious chpater. In this chapter, we first specify a setting for the interpolation and categorize the {{IM}}s into those that make fundamental shape assumptions (parametric) and those that are more flexible (non-parametric). We give an introduction for each method with a compact definition, highlight adjustments or give remarks where appropriate, and point out strengths and weaknesses of each method. A brief overview of the considered {{IM}}s is provided in table~\ref{table:pros_cons_overview}.
		% In this section, we take a closer look at several {{IM}}s, which will be used to interpolate and smooth the NDVI {TS}, while considering only SCL45 in this chapter. First, we define the general setting and discuss a general approach to make the interpolation more robust (i.e., reduce the impact of outliers). Afterwards, we introduce and discuss each method.
		Afterwards, we extract a robustification strategy from one {{IM}} and generalize it, so we can use it for all methods that allow for a priori weighted observations. Finally, using LOOCV, we tune the parameters (where necessary) and get a first idea of the performance of each method.


	}
	{% pros & cons table
		\footnotesize
		\input{tex/chapters/misc/pros_cons_overview.tex}
		\normalsize
	}



\section{Interpolation Setup}{\label{sec:itpl_setup}
	For now, we will only consider SCL45 observations since they are more reliable. Hence, data in the form of $\left(t_{i}, y_{i}\right)$ for $i=1, \ldots, n$ is given, where $t_i$ is the time in GDD and $y_i$ denotes the NDVI at $t_i$. We assume that it can be represented by
	$$
		y_{i}=m\left(t_{i}\right)+\varepsilon_{i},
	$$
	where $\varepsilon_i$ is some random noise and $m: \R \rightarrow \R$ is some (parametric or non-parametric) function. If we assume that $\varepsilon_{1}, \ldots, \varepsilon_{n}$ i.i.d. with $\mathbb{E}\left[\varepsilon_{i}\right]=0$ then 
	$$
		m(t)=\mathbb{E}[y \mid t]
	$$
	We will introduce parametric and non-parametric approaches to estimate $m$ in section~\ref{sec:itpl_parametric} and~\ref{sec:itpl_nonparametric}.
	Furthermore, in the subsequent, we denote $w\in \R^n$ as the vector of weights such that $w_i$ corresponds to the weight that $(t_i, y_i)$ should have in the interpolation. Since not all the following IMs support extrapolation outside the data range, we will always extrapolate with the closest value within the data range. 
}



\input{tex/chapters/misc/interpolation_methods.tex}


% \subsection{Other Methods to study:}
% From introduction of \cite{chenSimpleMethodReconstructing2004a}:\\
% (1) threshold-
% based methods, such as the best index slope extraction
% algorithm (BISE) (Viovy et al., 1992); (2) Fourier-based
% fitting methods (Cihlar, 1996; Roerink et al., 2000; Sellers
% et al., 1994); and (3) asymmetric function fitting methods
% such as the asymmetric Gaussian function fitting approach
% (Jonsson Eklundh, 2002) and the weighted least-squares
% linear regression approach (Swets et al., 1999).

\section{Tuning Parameter Estimation}{ \label{sec:itpl_param_est}
	Many of the {{IM}}s introduced in section~\ref{sec:itpl_parametric} and~\ref{sec:itpl_nonparametric} include a free parameter. To determine this parameter for a specific {{IM}}, we will estimate the absolute residuals using OOB estimation and then optimize the parameter using a score function. We clarify the procedure step by step:	
	\begin{Nenumerate}
		\item Construct a set $\Lambda$ of candidate parameters that generously covers the parameter space.
		% \item Consider $\mathcal{P}$, a set of Pixels.
		% \item For each parameter $\lambda \in \Lambda$ consider the individual pixels and the LOOCV\footnote{For a definition of the leave-one-out-cross-validation we refer to section~\ref{sec:OOB_LOOCV}.} estimator for the specific NDVI {{IM}} for all Pixels in $\mathcal{P}$ simulatnously. 
		% \item Determine $\lambda_{optimal} = \argmin_{\lambda\in\lambda}q_{90}(R_\lambda)$, where we describe the 90\% quantile with $q_{90}$.
		\item Consider the vector $z:=(y_1^{(1)},\dots,y_{n_1}^{(1)},y_1^{(2)},\dots,y_{n_2}^{(2)},\dots,y_1^{(m)},\dots,y_{n_m}^{(m)},)$ of all NDVI TS for all $m$ pixels.
		\item Consider its LOOCV estimator $\hat z_\lambda$ using the specific IM and the parameter $\lambda$ for each pixel individually.
		\item Determine $\lambda_{optimal} = \argmin_{\lambda\in\lambda}\operatorname{QAR}^{90}(\hat z_\lambda)$
	\end{Nenumerate}
	We choose $\operatorname{QAR}^{90}$ as our optimization function because we want to allow 10\% of outliers (contaminated points) but also aim for an accurate fit in 90\% of the cases.  
	Figure~\ref{fig:interpol/statistics_SS_param_optim} exemplifies the effect of the chosen quantile in the optimization function (QAR\textsuperscript{x}). To summarize, we may say that the higher the quantile, the stronger the smoothing. 

	\begin{my_figure}[h]{width=1\textwidth}{interpol/statistics_SS_param_optim}
		\caption[Smoothing splines optimized by minimizing the given quantile of the absolute leave-one-out residuals]{Smoothing splines fit with smoothing parameter optimized by minimizing the LOOCV QAR. Note that the larger the considered quantile is, the more the curvature of the resulting curve decreases.}
		\label{fig:interpol/statistics_SS_param_optim}
	\end{my_figure}

}


\section{Robustification}{
	\label{sec:loess_robustify}
	{ % Intro
		Now we discuss a general approach of how to make an interpolation more robust against outliers. The main idea is to give less weight to observations that have high residuals after the initial (or if we reiterate, the previous) fit.
		Even though the procedure is taken from the robust version of the LOESS smoother (cf. section~\ref{sec:loess} and \cite{clevelandRobustLocallyWeighted1979}), we can apply it to every {{IM}} that allows for prior weighting of observations.
	}
	{	
		After an initial fit, we calculate the residuals $r_i := y_i - \hat y_i$ and obtain $\tilde r_i$ by scaling with the median of the absolute residuals: 
		\begin{equation}
			\tilde r_i := \frac{r_i}{6\operatorname{med}\left(|r_1|,\dots,|r_n|\right)}
		\end{equation}
		Next, we compute new weights by
		\begin{equation}
			w_i^\text{new}:=w_i^\text{old} \begin{cases}
				\left(1-\tilde r_i^{2}\right)^{2}, & \text{if } |\tilde r_i|<1 \\
				0,                        & \text{else }
			\end{cases}.\quad
			\label{eq:bisquare}
		\end{equation}
		Using the new weights, we can re-interpolate. This reweighting can be iterated for several steps or till the change of the values is smaller than some tolerance.
	% Old more complicated version
		% Before we describe the procedure, we define a function that will determine the weight given to each observation, such that observations with large-scaled residuals will have less weight. That is the bisquare function B:
		% 	$$
		% 		B(x):=\begin{cases}
		% 			\left(1-x^{2}\right)^{2}, & \text{if } |x|<1 \\
		% 			0,                        & \text{else }
		% 		\end{cases}
		% 	$$ 
		% 	Now, we do something similar to what is done in iteratively reweighted least squares. After an initial interpolation, update the weights of each observation with
		% 	\begin{equation}
		% 		w_i^\text{new}:=w_i^\text{old} \operatorname{B}\left(     \right);\quad
		% 		r_i := y_i - \hat y_i
		% 		\label{eq:bisquare}
		% 	\end{equation}
		% 	and interpolate again using the new weights. We can iterate this reweighting and stop after several steps or when the change of the values is smaller than some tolerance.
	}
	This procedure is indeed robust since we use the median for the normalization which has a breakdown point\footnote{Intuitively, the breakdown point denotes the fraction of observations a `vicious' player can replace without breaking the estimator. For example, the median has a breakdown point of $50 \%$.} of $50 \%$.\footnote{The breakdown point relates only to outliers in the $y$ values. Note that we do not require the {{IM}}s to be robust, since the residual for an outlier will  still be larger than for non-outliers and thus will be down weighted more and more in each iteration (because for the next iteration the residual of the outlier will be even larger, since we gave less weight to it).}
	\subsection{Our Adjustment:}{
		During the iterations or when supplying prior weights, low-weighted observations can corrupt our estimation of scale (the median of absolute residuals). Thus, we will use the weighted median defined by
		$$
			\med_\text{weighted}(r,w) := \argmin_{\lambda \in \R} \sum_{i=1}^n |r_iw_i -\lambda|,
			\quad\text{ for }r,w\in \R^n.
		$$
		
	}
	\subsection{Examples and Conclusions}{		
		Examples of the first four iterations using SS and LOESS are shown in figure~\ref{fig:interpol/2x3_SS_robust} and~\ref{fig:interpol/2x3_loess_robust} for six pixels. For the analogous figures of BS and DL, we refer to the appendix~\ref{fig:interpol/2x3_B-Splines_robust} and~\ref{fig:interpol/2x3_DL_robust}.
		Indeed, we observe how the interpolated {TS} is less affected by outliers after each iteration. The most notable change happens during the first iteration. In figure~\ref{fig:interpol/2x3_SS_robust} plot (d) we see how the interpolation `escapes' from the right endpoint with each successive iteration, even though our intuition does not necessarily identify this point as an outlier. Therefore, in the following, we will always stop after one iteration.

		\begin{my_figure}[h]{width=1\textwidth}{interpol/2x3_SS_robust}
			\caption[Smoothing splines robustification.]{Smoothing splines \RobItPlot}
			\label{fig:interpol/2x3_SS_robust}
		\end{my_figure}
		\begin{my_figure}[h]{width=1\textwidth}{interpol/2x3_loess_robust}
			\caption[The LOESS smoother robustification.]{The LOESS smoother \RobItPlot}
			\label{fig:interpol/2x3_loess_robust}
		\end{my_figure}
	} 
	
	\subsection{Upper Envelope Approach}
		If we artificially increase the negative residuals in~\refeq{eq:bisquare} (e.g.,  by multiplying with 2), the corresponding points will get less weight in the next iteration. This allows us to create an interpolation that resembles an upper envelope. Intuitively, this upper envelope can be thought of as a sheet that is laid on top of the points.
			
		This approach is based on the premise that we tend to underestimate the NDVI \citep{caoSimpleMethodImprove2018b}. Since we want to develop a general method that is in principle not related to the NDVI, we will not pursue this approach further.	
}
\section{Performance Assessment}{\label{sec:itpl_perfomance_assessment}
	Next, we will benchmark the in section~\ref{sec:itpl_preselection} preselected {{IM}}s with and without robustification. For this, we will use the same technique as we did for the parameter determination in section~\ref{sec:itpl_param_est}. On $B_\lambda$ we apply the score functions from section~\ref{sec:scorefun}.  

	The results are presented in section~\ref{sec:results_itpl} and are discussed in section~\ref{sec:discussion_itpl}. The double logistic turns out to be the best convincing parametric method, and from the non-parametric methods we choose the SS.
}


