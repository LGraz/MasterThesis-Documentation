% \begin{my_pros_cons_table}{
%         \item 1
%         \item 2
%     }{
%         \item 1
%         \item 2
%     }
% \end{my_pros_cons_table}
\newcommand{\RobItPlot}{fitted to different (SCL45) NDVI time series. Iterations of a robustifing refit (as indicated in section~\ref{sec:loess_robustify}) are also displayed}


\chapter{Interpolation Methods} \label{sec:itpl}\todo{verdeutliche dem leser, dass ein auftrag das findne von interpolationmethoden war}
	{% Roadmap
		In section \ref{sec:s2_challangges} we have established the need for interpolating the NDVI time series. In this chapter we first specify a setting for the interpolation and divide the interpolation methods into those that make fundamental shape assumptions (parametric) and those that are more flexible (non-parametric). We give an introduction for each method with an compact definition, highlight adjustments or give remarks where appropriate, and then point out strengths and weaknesses of each method. Additionally, a brief overview of the considered interpolation methods is provided in table~\ref{table:pros_cons_overview}.
		% In this section, we take a closer look at several interpolation methods, which will be used to interpolate and smooth the NDVI time series, while considering only SCL45 in this chapter. First, we define the general setting and discuss a general approach to make the interpolation more robust (i.e. reduce the impact of outliers). Afterwards, we introduce and discuss each method.
		Afterwards, we extract an robustification strategy from the one interpolation method and generalize it so we can use it for all methods that allow for a priori weighted observations. Finally, using LOOCV, we tune the parameters (where necessary) and get a first idea of the performance of each method.


	}
	{% pros & cons table
		\footnotesize
		\input{tex/chapters/misc/pros_cons_overview.tex}
		\normalsize
	}



\section{Interpolation Setup}{\label{sec:itpl_setup}
	In this chapter, we will only consider SCL45 observations, since they are more reliably. Hence, data in the form of $\left(t_{i}, y_{i}\right)$ for $i=1, \ldots, n$ is given, where $t_i$ is the time in GDD and $y_i$ denotes the NDVI at time $t_i$. Assume that it can be represented by
	$$
		y_{i}=m\left(t_{i}\right)+\varepsilon_{i},
	$$
	where $\varepsilon_i$ is some noise and $m: \R \rightarrow \R$ is some (parametric or non-parametric) function. If we assume that $\varepsilon_{1}, \ldots, \varepsilon_{n}$ i.i.d. with $\mathbb{E}\left[\varepsilon_{i}\right]=0$ then 
	$$
		m(t)=\mathbb{E}[y \mid t]
	$$
	We will introduce parametric and non-parametric approaches to estimate $m$ in section \ref{sec:itpl_parametric} and \ref{sec:itpl_nonparametric}
	Furthermore, in the subsequent, we denote $w\in \R^n$ as the vector of weights such that $w_i$ corresponds to the weight that $(t_i, y_i)$ should have in the interpolation. 
}


\todo[inline]{gewichte einf√ºgen}

\input{tex/chapters/misc/interpolation_methods.tex}


% \subsection{Other Methods to study:}
% From introduction of \cite{chenSimpleMethodReconstructing2004a}:\\
% (1) threshold-
% based methods, such as the best index slope extraction
% algorithm (BISE) (Viovy et al., 1992); (2) Fourier-based
% fitting methods (Cihlar, 1996; Roerink et al., 2000; Sellers
% et al., 1994); and (3) asymmetric function fitting methods
% such as the asymmetric Gaussian function fitting approach
% (Jonsson Eklundh, 2002) and the weighted least-squares
% linear regression approach (Swets et al., 1999).

\section{Tuning Parameter Estimation}{ \label{sec:itpl_param_est}
	Many of the interpolation methods introduced in section \ref{sec:itpl_parametric} and \ref{sec:itpl_nonparametric} include a free parameter. To determine this parameter for a specific interpolation method, we will estimate the absolute residuals using OOB estimation and then optimize the parameter using a score function. We clarify the procedure step by step:	
	\begin{Nenumerate}
		\item Construct a set $\Lambda$ of candidate parameters that generously covers the parameter space.
		\item Consider $\mathcal{P}$, a set of Pixels.
		\item For each parameter $\lambda \in \Lambda$ consider the individual pixels and compute the LOOCV\footnote{For a definition of the leave-one-out-cross-validation we refer to section~\ref{sec:OOB_LOOCV}} for the absolute residuals of the specific NDVI interpolation method for all Pixels in $\mathcal{P}$ and store them in the set $R_\lambda$. 
		\item Determine $\lambda_{optimal} = \argmin_{\lambda\in\lambda}q_{90}(R_\lambda)$, where we describe the 90\% quantile with $q_{90}$.
	\end{Nenumerate}

	\begin{my_figure}[h]{width=1\textwidth}{interpol/statistics_SS_param_optim}
		\caption{Smoothing splines fit with smoothing parameter optimized by minimizing the given quantile of the absolute leave-one-out residuals. Note that the larger the considered quantile is, the smoother the resulting curve becomes.}
		\label{fig:interpol/statistics_SS_param_optim}
	\end{my_figure}

	We choose $\operatorname{quantile}(90)$ as our optimization function because we want to allow 10\% of outliers (corrupt points) but also aim for an accurate fit in 90\% of the cases.  
	
	Figure \ref{fig:interpol/statistics_SS_param_optim} exemplifies the effect of the optimization function (different quantiles). To summarize, we may say that the higher the quantile, the stronger the smoothing. 
}


\section{Robustification}{
	\label{sec:loess_robustify}
	{ % Intro
		Now we discuss a general approach of how to make an interpolation more robust against outliers. The main idea is to give less weight to observations that have high residuals after the initial (or if we reiterate, the previous) fit.

		Even though the procedure is taken from the robust version of the LOESS smoother (c.f. section~\ref{sec:loess} and \cite{clevelandRobustLocallyWeighted1979}), we can apply it to every interpolation method that allows for prior weighting of observations.
	}
	
	{	
		After an initial fit we calculate the residuals $r_i := y_i - \hat y_i$ and obtain $\tilde r_i$ by scaling with the median of the absolute residuals: 
		\begin{equation}
			\tilde r_i := \frac{r_i}{6\operatorname{med}\left(|r_1|,\dots,|r_n|\right)}
		\end{equation}
		Next, we compute new weights by
		\begin{equation}
			w_i^\text{new}:=w_i^\text{old} \begin{cases}
				\left(1-\tilde r_i^{2}\right)^{2}, & \text{if } |\tilde r_i|<1 \\
				0,                        & \text{else }
			\end{cases};\quad
			\label{eq:bisquare}
		\end{equation}
		Using the new weights, we can re-interpolate. This reweighting can be iterated for several steps or till the change of the values is smaller than some tolerance.

	% Old more complicated version
		% Before we describe the procedure, we define a function that will determine the weight given to each observation, such that observations with large-scaled residuals will have less weight. That is the bisquare function B:
		% 	$$
		% 		B(x):=\begin{cases}
		% 			\left(1-x^{2}\right)^{2}, & \text{if } |x|<1 \\
		% 			0,                        & \text{else }
		% 		\end{cases}
		% 	$$ 
		% 	Now, we do something similar to what is done in iteratively reweighted least squares. After an initial interpolation, update the weights of each observation with
		% 	\begin{equation}
		% 		w_i^\text{new}:=w_i^\text{old} \operatorname{B}\left(     \right);\quad
		% 		r_i := y_i - \hat y_i
		% 		\label{eq:bisquare}
		% 	\end{equation}
		% 	and interpolate again using the new weights. We can iterate this reweighting and stop after several steps or when the change of the values is smaller than some tolerance.
	}

	Note that this procedure is indeed robust since we use the median for the normalization which has a breakdown point\footnote{Intuitively, the breakdown point denotes the fraction of observations a ``vicious'' player can replace without breaking the estimator. For example, the median has a breakdown point of $50 \%$.} of $50 \%$.\footnote{The breakdown point relates only to outliers in the $y$ values. Note that we do not require the interpolation methods to be robust, since the residual for an outlier will  still be larger than for non-outliers and thus will be down weighted more and more in each iteration (because for the next iteration the residual of the outlier will be even larger, since we gave less weight to it).}
	\subsection{Our Adjustment:}{
		During the iterations or when supplying prior weights, low-weighted observations can corrupt our estimation of scale (the median of absolute residuals). Thus, we introduce the weighted median as
		$$
			\med_\text{weighted}(r,w) := \argmin_{\lambda \in \R} \sum_{i=1}^n |r_iw_i -\lambda|
		$$
		for $r,w\in \R^n$. 
	}
	\subsection{Examples and Conclusions}{
		\begin{my_figure}[h]{width=1\textwidth}{interpol/2x3_SS_robust}
			\caption{Smoothing Splines \RobItPlot}
			\label{fig:interpol/2x3_SS_robust}
		\end{my_figure}

		Examples of the first four iterative fits using smoothing splines are shown in figure \ref{fig:interpol/2x3_SS_robust} for six pixels. For the analogous figures of the other interpolation methods c.f. figures \ref{fig:interpol/2x3_loess_robust}, \ref{fig:interpol/2x3_B-Splines_robust}, \ref{fig:interpol/2x3_DL_robust} and \ref{fig:interpol/2x3_loess_robust}.
		Indeed, we observe how the interpolated time series is less affected by outliers after each iteration. We notice the biggest difference in the first iteration. Furthermore, in the plot at the bottom left\todo{ consider naming the sub-plots} we see how the interpolation `escapes' from the right endpoint with each successive iteration, even though our intuition does not necessarily identify this point as an outlier. Therefore, in the following, we will always stop after one iteration.
	} 
	
	\subsection{Upper Envelope Approach - Penalty for Negative Residuals}
		If we artificially increase the negative residuals in \refeq{eq:bisquare} by multiplying (e.g. factor 2), the corresponding points will get less weight in the next iteration. This allows us to create an interpolation that resembles an upper envelope. Intuitively, this upper envelope can be thought of as a sheet that is laid on top of the points.
			
		This approach is based on the premise that we tend to underestimate the NDVI (as argued in \cite{caoSimpleMethodImprove2018b}). Since we want to develop a general method that is in principle not related to the NDVI, we will not pursue this approach further.	
}
\section{Performance Assessment}{
	Next, we will benchmark the different interpolation methods with and without robustification. For this, we will use the same technique as we did for the parameter determination in section \ref{sec:itpl_param_est}. On $B_\lambda$ we apply the RMSE and different quantiles.  

	The results are presented in section \ref{sec:results_itpl} and are discussed in section \ref{sec:discussion_itpl}. The double logistic turns out to be the best convincing parametric method and from the non-parametric methods we choose the smoothing splines.
}


