% \begin{my_pros_cons_table}{
%         \item 1
%         \item 2
%     }{
%         \item 1
%         \item 2
%     }
% \end{my_pros_cons_table}
\newcommand{\RobItPlot}{fitted to different (SCL45) NDVI time series. Iterations of a robustifing refit (as indicated in section~\ref{sec:loess_robustify}) are also displayed}


\chapter{Interpolation Methods} \label{sec:itpl}
	{% Roadmap
		In this section, we take a closer look at several interpolation methods, which will be used to interpolate and smooth the NDVI time series. 
		A brief overview over the considered interpolation methods can be found in table~\ref{table:pros_cons_overview}.

		First, we define the general setting and discuss a general approach to make the interpolation more robust (i.e. reduce the impact of outliers).

		Afterwards, we introduce and discuss each method.

		Then, we try to extract the main ingredients of each method to forge our own one.

		Finally, using leave-one-out cross validation, we tune the parameters (where necessary) and get a first idea of the performance of each method.
	}
	{% pros & cons table
		\footnotesize
		\input{tex/chapters/misc/pros_cons_overview.tex}
		\normalsize
	}

\section{DAS vs. GDD}{
	%example plot
	Prior to interpolating the NDVI time series, we should decide on a time scale. We can choose between DAS and GDD (cf. section \ref{sec:gather_data_to_pixel} and equation \ref{eq:gdd}). In figure \ref{fig:interpol/das_vs_gdd} we see an example for comparison of the two. Here we see that the first 120 DAS are compressed to just 500 GDD. 
	This has several advantages. First, it makes the scales comparable (in terms of plant growth) because the plants are not concerned with the month of the year but the current temperature. Second, in winter we tend to have higher cloud cover and thus fewer SCL45 observations. Hence, this gap in observations is compressed. Consequently, we will only use GDD in the subsequent.

	\begin{my_figure}[h]{width=0.8\textwidth}{interpol/das_vs_gdd}
		\caption{The same NDVI timeseries, on the left with DAS as the time scale on the right GDD is the time scale. SCL45 are colord as black. Non-SCL45 (clouds and shadows) are colored in grey.}
		\label{fig:interpol/das_vs_gdd}
	\end{my_figure}
}

\section{Setting}{
	We are given data in the form of $\left(x_{i}, Y_{i}\right)$ for $i=1, \ldots, n$. Assume that it can be represented by
	$$
		y_{i}=m\left(x_{i}\right)+\varepsilon_{i},
	$$
	where $\varepsilon_i$ is some noise and $m: \R \rightarrow \R$ is some (parametric or non-parametric) function. If we assume that $\varepsilon_{1}, \ldots, \varepsilon_{n}$ i.i.d. with $\mathbb{E}\left[\varepsilon_{i}\right]=0$ then $$m(x)=\mathbb{E}[y \mid x]$$
	We will introduce some approaches to estimate $m$ in section \ref{sec:itpl_parametric} and \ref{sec:itpl_nonparametric}.

	Furthermore, in the subsequent we denote $w\in \R^n$ as the vector of weights such that $w_i$ corresponds to the weight that $(x_i, Y_i) should have in the interpolation$. 
}




\input{tex/chapters/misc/interpolation_methods.tex}


% \subsection{Other Methods to study:}
% From introduction of \cite{chenSimpleMethodReconstructing2004a}:\\
% (1) threshold-
% based methods, such as the best index slope extraction
% algorithm (BISE) (Viovy et al., 1992); (2) Fourier-based
% fitting methods (Cihlar, 1996; Roerink et al., 2000; Sellers
% et al., 1994); and (3) asymmetric function fitting methods
% such as the asymmetric Gaussian function fitting approach
% (Jonsson Eklundh, 2002) and the weighted least-squares
% linear regression approach (Swets et al., 1999).

\section{Tuning parameter estimation}{ \label{sec:itpl_param_est}
	Viele der in sektion \ref{sec:itpl_parametric} und \ref{sec:itpl_nonparametric} eingeführten interpolationsmethoden beinhalten einen freien parameter. Um diesen für eine spezifische interpolationmethode auszuwählen werden wir mithilfe OOB schätzung die absoluten residuen schätzen und dann anhand einer statistik den parameter optimieren. Wir klären die vorgehensweise schritt für schritt:	
	- konstruiere eine menge $\Lambda$ von kanditat-parameter, welche den parameterraum großzügig abdeckt.
	- betrachte $\mathcal{P}$, eine Menge von pixeln
	- für jeden parameter $\lambda \in \Lambda$ betrachte die einzelnen pixel und berechne die leave-one-out-cross-validation (\textit{LOOCV}) für den NDVI, mit der spezifischen interpolationsmethode. D.h. $\hat y_i$ erhalten wir durch die interpolation von den punkten $\left\{ (x_j,y_j)|j\neq i \right\}$. Anschließend berechne die absoluten residuen $|y_i - \hat y_i|$ und definiere die Menge $R_\lambda$ als die menge aller residuen (aller pixel in $\mathcal{P}$).
	- bestimme $\lambda_{optimal} = \argmin_{\lambda\in\Lambda}\operatorname{Quantile}(90)(R_\lambda)$, wobei wir mit $\operatorname{Quantile}(90)$ das 90\% quantil beschreiben.

	\begin{my_figure}[h]{width=1\textwidth}{interpol/statistics_SS_param_optim}
		\caption{Smoothing splines fit with smoothing parameter optimized by minimizing the ``\dots''-quantile of the absolute leave-one-out residuals. Note that the larger the considered quantile is, the smoother the resulting curve becomes.}
		\label{fig:interpol/statistics_SS_param_optim}
	\end{my_figure}

	Wir haben uns für $\operatorname{Quantile}(90)$ entschieden als optimierungsfunktion, wir zum einem einen anteil von 10\% an ausreißern erlauben wollen (korrupte punkte) aber zum anderen in 90\% der fälle einen akkuraten fit anstreben. 
	
	In der Abbildung \ref{fig:interpol/statistics_SS_param_optim} sehen wir beispielhaft wie die interpolationen aussehen wenn wir zur parameterbestimmung andere quantile benutzen. Zusammengefasst kann man sagen, dass je höher das quantil desto stärker auch die glettung. 
}


\section{Robustify}{
	\label{sec:loess_robustify}
	{ % Intro
		Now we discuss a general approach of how to make an interpolation more robust against outliers. The main idea is to give less weight to observations which have high residuals after the initial (or if we reiterate, the last) fit.

		Even though the procedure is taken from the robust version of the LOESS smoother (cf. section~\ref{sec:loess} and \cite{clevelandRobustLocallyWeighted1979}), we can apply it to every interpolation method that allows for prior weighting of observations.
	}
	
	{	
		Before we describe the procedure, we define a function which will determine the weight given to each observation such that observations with large scaled residuals will have less weight. That is the bisquare function B:
		$$
			B(x):=\begin{cases}
				\left(1-x^{2}\right)^{2}, & \text{if } |x|<1 \\
				0,                        & \text{else }
			\end{cases}
		$$

		Now, we do something similar to what is done in iteratively reweighted least squares. After an initial interpolation, update the weights of each observation with
		\begin{equation}
			w_i^\text{new}:=w_i^\text{old} \operatorname{B}\left(\frac{|r_i|}{6\operatorname{med}\left(|r_1|,\dots,|r_n|\right)}\right);\quad
			r_i := y_i - \hat y_i
			\label{eq:bisquare}
		\end{equation}
		and interpolate again using the new weights. We can iterate this reweighting and stop after several steps or when the change of the values is smaller than some tolerance.
	}

	Note that this procedure is indeed robust since we use the median for the normalization which has a breakdown point of $50 \%$.\footnote{The breakdownpoint relates only to outliers in the $y$ values. Note that we do not require the interpolation methods to be robust, since the residual for an outlier will  still be larger than for non-outliers and thus will be downweighted more and more in each iteration (because for the next iteration the resiudal of the outlier will be even larger, since we gave less weight to it).}
	\subsection{Our Adjustment:}{ 
		In the case that we would like to apply prior weights, we want to prevent low-weighted observations to corrupt our estimation of scale (the median) and thus we use the weighted median. This can be defined as
		$$
			\med_\text{weighted}(r,w) := \argmin_{\lambda \in \R} \sum_{i=1}^n |r_iw_i -\lambda|
		$$
		for $r,w\in \R^n$. \footnote{This adjustment is also necessary to keep the scale estimation meaningful during the iterations.}
	}
	\subsection{Examples and Conclusions}{
		\begin{my_figure}[h]{width=1\textwidth}{interpol/2x3_SS_robust}
			\caption{Smoothing Splines \RobItPlot}
			\label{fig:interpol/2x3_SS_robust}
		\end{my_figure}

		In abbildung \ref{fig:interpol/2x3_SS_robust} sehen wir exemplarisch für 6 pixel wie die mit smoothing splines interpolierte NDVI zeitreihe nach $0,1,2,3,4$ iterationen aussieht (für die analogen abbildungen der andere interpolations methoden verweisen wir auf den Anhang: \ref{fig:interpol/2x3_loess_robust}, \ref{fig:interpol/2x3_B-Splines_robust}, \ref{fig:interpol/2x3_DL_robust} and \ref{fig:interpol/2x3_loess_robust}).
		
		In der tat beobachten wir, wie die interpolierte zeitreihe nach jeder interation weniger durch ausreißer beinflusst wird. Den grössten unterschied beobachten wir bei der ersten iteration. Ausserdem sehen wir im plot unten links wie die Interpolation dem rechten endpukt mit jeder weiteren iteration ``entflieht'', obwohl unsere intuition diesen punkt nicht unbedingt als ausreißer identifiziert. Daher werden wir im folgenden immer nur eine iteration durchführen und dannach aufhören.
 
	} 
	
	\subsection{Upper Envelope Approach - Penalty for negative resiudals}
		Wenn wir in \refeq{eq:bisquare} die negativen residuen durch multiplikation (z.B. faktor 2) künstlich erhöhen, so werden die korrespondierenden punkte in der nächsten iteration weniger gewicht erhalten. Dies ermöglicht uns eine interpolation zu erzeugen, welche einer oberen hülle gleicht. Diese obere hülle kann man sich intuitiv als ein tuch vorstellen, welches man von oben auf die punkte legt.
		
		Dieses vorgehen beruht der annahme , dass wir den NDVI tendenziell unterschätzen (wie in REF-savitzky-golay). Da wir eine generelle methode entwickeln wollen, welche prinzipiell nicht and den NDVI gekoppelt ist, werden wir diesen Ansatz nicht weiter verfolgen.

}
\section{Performance Assecement}{
	Nun wollen wir die verschienden interpolationsmethoden einmal mit und einmal ohne robustifizierung benchmarken. Dafür werden wir die gleiche technik verwenden, wie wir sie bei der parameterbestimmung in sektion \label{sec:itpl_param_est}. Auf $B_\lambda$ wenden wir den RMSE und verschiedene quantile an und stellen die Ergebnisse in tabelle \ref{tab:cv-statistics_itpl-methods} dar. 


	\begin{table}
		\begin{center}
			\caption{Performance comparison of different interpolation methods measured with various statistics. Considering only SCL45 points, we get the out-of-bag estimates using the given interpolation method. Consequently, we compute the absolute (value of the) residuals and apply the given statistic to it.}
			\small
			\input{tex/chapters/misc/table_cv-statistics_itpl-methods}
			\normalsize
			\label{tab:cv-statistics_itpl-methods}
		\end{center}
	\end{table}
}

\section{XXX Evaluation}
-- ss dominate (i.e. have better benchamrk values w.r.t. all considered statistics) b-splines (robustified and non-robustified)

-- dl dominate fourier (robustified and non-robustified)

-- loess slightly dominates ss, but we prefere ss because of the smoothnes guarantees (compare the figures \ref{fig:interpol/2x3_loess_robust} and \ref{fig:interpol/2x3_SS_robust}).

\section*{TEMP --- Figures}
\begin{my_figure}[h]{width=0.6\textwidth}{interpol/res_cv}
	\caption{XXX caption XXX}
	\label{fig:interpol/res_cv}
\end{my_figure}


