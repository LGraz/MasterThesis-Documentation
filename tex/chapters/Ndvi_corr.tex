\chapter{NDVI Correction / Improve NDVI Data}

{
    Let's remind ourselves that the data from the Sentinel-2 is equipped with a scene classification layer (SCL) and we therefore have some information of what is observed at each pixel for each sampled time (cf. table~\ref{tab:satelite/scl_classes}). In this chapter we would like to improve the observed NDVI values by using more information than just the two bands used to calculate the NDVI (B4 and B8).
}

\section{Considering other SCL Classes}{
    In figure ~\ref{fig:ndvi_corr/residuals_scl_classes} we see for example that some blue points\footnote{The blue points correspond to the SCL-class 10: Thin cirrus clouds} follow the interpolated line closely and that they might be useful in improving an interpolation fit.

    \begin{my_figure}[ht]{width=0.6\textwidth}{ndvi_corr/residuals_scl_classes}
        \caption{A smoothing splines fit considering green and yellow points (SCL45)}
        \label{fig:ndvi_corr/residuals_scl_classes}
    \end{my_figure}

    To get an impression whether there is some useful information contained in the remaining SCL-classes (all except 4 and 5) we would like to compare the observed NDVI with the true NDVI. But since we do not have any ground truth data, we will make the following assumption:


    \begin{definition}{XXXAssumption (true NDVI)}
        The true NDVI value at time $t$ can be successfully estimated by out-of-bag interpolation using high quality observations. That is the interpolated value (using XXX) considering the points $P^{SCL45}\setminus P_t$. In the following, we will call this estimate the ``true''-NDVI
    \end{definition}

    shall pair every observed NDVI value with its out-of-bag-estimate. Then for each category we collect all pairs and create a scatter plot in fig~\ref{fig:ndvi_corr/scl_residuals_scatter}XXXXXXXXXXXXXX



    \begin{enumerate}
        \item For each pixel and for each observation (every SCL-class):\\
            estimate the NDVI value (via out-of-the-box interpolation\footnote{That is, we use all observations (in SCL45) but the current one.})
        \item
    \end{enumerate}







    \begin{my_figure}[h]{width=1\textwidth}{ndvi_corr/scl_residuals_scatter}
        \caption{XXX caption XXX}
        \label{fig:ndvi_corr/scl_residuals_scatter}
    \end{my_figure}
}



\section{XXX Correction}{
    roadmap ... (intuition, data-table, ml-methods, uncertainty, refit and evaluation)

    \subsection{XXX idea -and- stepwise plots}
    
    \subsection{XXX data-table-construction}
        \label{sec:corr_data_table}
        XXX discussion about choosen covariates:
        list of things we considered but rejected + reasoning --> no weather to keep it general even though we have it implemeted

    \subsection{XXX ml-methods}{
            %     ML-methoden: muss ich sie beschreiben? Es scheint mir wissen zu sein, was ich vorraussetzen kann. 
            % --> ja, kurz und bündig erklären
        \subsubsection*{Ordinary Least Squaers (OLS)}{
            The OLS is a linear model which aims to minimize the sum of the squared residuals. Let $y\in \R^n$ be the vector of responses and $X\in \R^{n\times p}$ be the design matrix, where each row corresponds to one pixel and each column consist of one covariate\footnote{Strictly speaking since SCL-classes are dummy variables }. We assume a linear relationship between $y$ and $X$ and allow for gaussian noise. That is:
            \begin{equation}
                \label{eq:ols}
                y = X\beta  + \epsilon \quad \text{ where }\epsilon \overset{i.i.d.}{\sim}\mathcal{N}(0,\sigma^2)
            \end{equation}
            Assuming that $X$ is regular, we can estimte the regression coefficients $\beta$ by
            \begin{equation}
                \hat \beta = (X^TX)^{-1}X^Ty = \argmin_{\beta in \R^p}\|y - X\beta\|_2^2 
            \end{equation}

            We will train two models, one using only the SCL-classes as covariates and the other one using all covariates (which are discussed in section \ref{sec:corr_data_table}).

            \begin{my_pros_cons_table}{
                \item Simple method with good interpretability of coefficients.
            }{
                \item Catches only linear relationships.
                \item No integrated variable selection.\footnote{There is the possibility of stepwise modelselection by dropping or adding some covariates but for higher p this gets too expensive since the time complexity grows in $\mathcal{O}(np^4)$ (computing $X^TX$ requires $\mathcal{O}(np^2)$ which assuming $n>p$ dominates $\mathcal{O}(p^3) $ needed for the cholesky-decomposition of $X^TX$) % p^2 time OLS. 
                }
            }
            \end{my_pros_cons_table}
        }
        \subsubsection*{LASSO}{
            The Lasso can be similarily expressed than the OLS but adds a penalty to the minimization problem:
            \begin{equation}
                \hat \beta_\lambda = \argmin_{\beta \in \R^p}\|y - X\beta\|_2^2 + \lambda \|\beta\|_1 = \argmin_{\beta \in \R^p \text{ and } \|\beta\|_1<\lambda}\|y - X\beta\|_2^2.\footnote{The last two terms are equivalent by lagrangian optimization}
                \label{eq:lasso}
            \end{equation}
            Even though we do not have a closed form solution for equation \ref{eq:lasso} we can solve it easily via optimization, since the function $\beta \in \{\beta\in\R^p|\|\beta\|_1<\lambda\}\|\ \mapsto \|y - X\beta\|_2^2$  is continious and convex.

            \cite{tibshiraniRegressionShrinkageSelection2011} shows that the LASSO solution tends to be sparse (for not to big $\lambda$). That is $\beta_i = 0$ for most $i = 1,\dots,p$

            In order to know which $\lambda$ to choose we try a huge range of possible values. For each $\beta_\lambda$ we calculate the cross-validated $RMSE_\lambda$
            \footnote{The cross-validatet Root Mean Square Error is the mean of the RMSE's obtained for each fold (using the model trained on the remaining folds). 
            We use the following definition of the $RMSE$: $\sqrt{\sum_{i=1}^n(y-\hat y)^2/n}$
            } (and its standard deviation $\sigma_\lambda$ using the $k$ folds) and define the $\lambda$ with the smallest corresponding  $RMSE_\lambda$ as $\lambda_{min}$. From here we choose the largest $\lambda$ for which the $RMSE_\lambda$ is smaller than $RMSE_{\lambda_min}+\sigma_\lambda$. This yields a simpler model while keeping the $RMSE$ reasonable model.

            We will apply the Lasso using the selected covariates in section \ref{sec:corr_data_table} and their first degree of interactions.\footnote{This is if our covariates are $\{a,b\}$, then we will now use $\{a,b,ab,a^2,b^2\}.$}
            
            \begin{my_pros_cons_table}{
                \item Usually yields a sparse solution. This tends to give better generalizability (prediction performance on unseen data).
                \item Successfully deals with correlation in covariates. 
                \item interpretable results
            }{
                \item Estimate is biased.
                \item Computationally expensive.
            }
            \end{my_pros_cons_table}
        }
        \subsubsection*{Random Forest}{
            XXX
            \begin{my_pros_cons_table}{
                \item xxx
            }{
                \item xxx
            }
            \end{my_pros_cons_table}
        }
        \subsubsection*{Multivariate Adaptive Regression Splines (MARS)}{
            XXX
            \begin{my_pros_cons_table}{
                \item xxx
            }{
                \item xxx
            }
            \end{my_pros_cons_table}
        }
        \subsubsection*{General Additive Model (GAM)}{
            XXX
            \begin{my_pros_cons_table}{
                \item xxx
            }{
                \item xxx
            }
            \end{my_pros_cons_table}
            }

    }
    
    \subsection{XXX Uncertainty}
    abs(residuals), train models for uncertainty, estimate residuals, get weights (via weight-function)  (problem of weight function -> we should norm the weights somehow since smoothing parameters are ``dependent'' on weights -> then, some outer points get really low weights (just because others in the middle have very little residuals and thus very high weight))
    
}



\section{XXX Evaluation Method}{
    {
        yield estimation is a main goal. 
        Claim that yield-estimation-accuracy is a objective measure :
            - we have not looked at the yield so far 
            - if the one NDVI-time-series predicts the yield better than a different one, we conclude that the first time-series carries more true information about the plants
        Now: "yield ~ NDVI-TS / derived-covariates" 
    }
    
    \subsection{yield estimation}{
        problem: high dimensionality and unequal duration/length -> use features
        
        name approaches for yield estimation (we will use a simple but flexible one)
        
        random forest >> for evaluation out-of-bag estimates
    }
    \subsubsection{Covariates used}{
        reference to kamir et al, why we did choosed some and not others
    }
    
}



%satelite/time_series_2021_P112/35_scl4_2021-06-03.png 
%satelite/time_series_2021_P112/40_scl10_2021-06-28.png 
%satelite/time_series_2021_P112/30_scl4_2021-05-09.png 
%satelite/time_series_2021_P112/15_scl5_2021-02-23.png 
%satelite/time_series_2021_P112/45_scl2_2021-07-23.png 
%satelite/time_series_2021_P112/33_scl9_2021-05-24.png
