\chapter{Discussion}\label{sec:discussion}

\section{{{IM}}s}{ \label{sec:discussion_itpl}
    \subsection{Data Gaps in Time Series}\label{sec:discussion_itpl_data_gaps}{
        NW estimates the value for $t$ by relating to the points near $t$. To determine what ``near'' means, a bandwidth $h$ is used (cf. equation~\refeq{eq:kernel_with_bandwidt}). This gets problematic as soon as the data gaps become larger than $h$, since in this case no points are left that are considered to be close to $t$. 

        Regarding the GK, we expect that due to the stationarity assumption, the interpolation will always tend to the mean if data gaps are present (cf. figure~\ref{fig:kriging_parameters}). 

        Since the SG requires equidistant points, it follows that data gaps will break it. The linear interpolation, that is supposed to recover this, we consider as not being a satisfying solution.

        % FS SS LOESS DL BSPL 
        We do not trust the FS interpolation if there are noticeable data gaps. On the one hand, it corresponds to our experience that the curve can escape strongly there (cf. figure~\ref{fig:interpol/fourier_dl_comparison}). On the other hand, the unreliability is illustrated by the poor values in table~\ref{tab:cv-statistics_itpl-methods} for the robustified variant. These are meaningful in describing the ability to cope with data gaps, since more data points are ignored during the robustification and thus data gaps are simulated. 

        Similarly, for SS, LOESS, DL and BS we compare the values in table~\ref{tab:cv-statistics_itpl-methods} between the robustified and non-robust variant. We find that the robust variant does not differ strongly from the non-robust variant (unlike as for FS). Thus, we conclude that these methods do not have systematic failures.

        Regarding the LOESS, we observe in the figure~\ref{fig:interpol/2x3_loess_robust} in plot (c) a strange peak between the first and second observation. This peak is due to the local weighting. In case of data gaps, the weights can attain non-intuitive values. For instance, the first data point in the plot, although adjacent to the peak, is given a low weight compared to the points to the right of the peak (for estimating the value at this peak).

        In our experience, the DL handles data gaps well, but it may happen that the model describes the NDVI increase as abrupt. This however was fixed, by bounding the first derivative (cf. section~\ref{sec:itpl_param_optimizationissues}).
    }

    \subsection{Preselection}{\label{sec:itpl_preselection}
        We shall now justify our preselection of the {{IM}}s tested in section~\ref{sec:itpl_perfomance_assessment}. 
        We decided against NW because it has systematic errors at peaks and valleys. Moreover, this method handles data gaps poorly (cf.~\ref{sec:discussion_itpl_data_gaps}). 
        Moreover, we will not consider UK since the underlying assumptions are not met and therefore a systematic bias is introduced. On top of that, maximum likelihood parameter estimation occasionally fails.
        Also, we do not include the SG in the next selection, since we see it as a special case of LOESS.
    }

    \subsection{Candidate Selection}{\label{sec:itpl_candiate_selection}
        Given that DL convinces regarding most of the selected score functions in table~\ref{tab:cv-statistics_itpl-methods} we will certainly investigate this method in chapter~\ref{sec:corr}. Moreover, we see that the robustification mostly improved the score regarding the 50, 75, 85, and 90 \% Quantiles. Only for the outlier-sensitive score functions (RMSE and q95)\footnote{For the RMSE one outlier is enough to take away the usefulness of the statics, in the case of q95 it is enough if 5\% of the data are corrupt to break the statics.} we notice significant worsening (we consider the robust FS separately in section~\ref{sec:discussion_itpl_data_gaps}). Consequently, we will also use the robustification in section~\ref{sec:corr}.
        In order to not only rely on the form assumptions of the DL, we further choose a non-parametric method for further consideration. Despite the LOESS slightly dominating the SS in table~\ref{tab:cv-statistics_itpl-methods}, we choose the SS. This is due to the strange behavior of the LOESS in case of data gaps (see section~\ref{sec:discussion_itpl_data_gaps}) and the good interpretability of the SS using the minimization function~\refeq{eq:ss}.
    }
}


\section{NDVI Correction}{\label{sec:discussion_corr}
    \subsection{Choose {{IS}}}\label{sec:discussion_iplfstrategy-choose}

    The evaluation of various ISs via the YPE (cf. section~\ref{sec:results_ndvi_corr}) clearly shows that SS are better suited than DL for yield estimation. Moreover, it seems surprising that robustification tends to lead to worse results, despite reducing LOOCV residuals in most cases (cf. section~\ref{sec:results_itpl}). We suspect that the correction models detect outliers to some extent and give them less weight. Indeed, for OLS\textsuperscript{SCL} in equation \refeq{eq:corr_lm_res} we see that the smaller the observed NDVI of a point, the larger the estimated residual and consequently a lower weight. This is consistent with our experience that outliers usually suggest a too small NDVI. This suspicion is consistent with the fact that if we do not correct, robustification produces a marginal improvement. 
    By using the best IS with correction (OLS\textsuperscript{SCL}), rather than the best IS without correction (SS\textsuperscript{rob}), we gain $(0.148-0.140)/0.148 = 5.4\%$ of information about the underlying vegetation --- that is, 100\% would allow us to model the yield perfectly.
    
    Note that the results discussed here depend strongly on the link function used. Once we change this, we should also repeat this analysis.
            


    \subsection{Investigation of Error Sources in Yield Estimation}{\label{sec:discuss_high-rmse-in-yield-prdiction}
        Although the YPE was not our primary goal, but was only used as a means to select the best IS, we compare our values with the corresponding ones of \cite{perichPixelbasedCropYield2022}. There, a non-relative YPE 1.00 [t/ha] was obtained using weather data in addition to NDVI TS. Since our error is only about 3.3\% larger (cf. table~\ref{tab:methods_vs_yieldprediction}), we consider our results to be quite competitive. Especially since we did not use meterological data aside from the time-scale transformation (cf. section~\ref{sec:gdd_def}). In the following we ask ourselves how much we can actually expect. We have multiple sources of uncertainty in the data:
        \begin{Nenumerate}
            \item Uncertainty in Yield data collected by the combine harvester.
            \item Uncertainty in Yield data through rasterization.
            \item Uncertainty in satellite images through ``measurement errors'' introduced via clouds and other atmospheric effects.
            \item Uncertainty introduced by interpolating (especially when long data-gaps are present).
        \end{Nenumerate}
        Furthermore, even if we would have a perfect NDVI curve, it contains only a fraction of the information about the underlying vegetation. 
        Nonetheless, \cite{perichPixelbasedCropYield2022} manages to explain up to 86\% of the variance in crop yield with only the NDVI TS and weather data (Table 5).  Although the authors divided the data into training and test data, this subdivision was done randomly at the pixel level (without subdividing into fields or years). Thus, there are pixels in the training data that are neighbouring pixels from the test data and thus exhibit high correlations (in yield and NDVI). The authors assume that the poor results for the cross-year-validation\footnote{By cross-year-validation we understand a cross validation with respect to the RMSE, where each year represents a single fold.} (table 6) are due to uneven (extreme) weather. We on the other hand, suspect that overfitting via high-correlation pixels is responsible for these high values. This could be investigated by performing a cross-field-validation\footnote{By cross-field-validation we understand a cross validation with respect to the RMSE with a partitioning $\mathcal{F}=\{F_1,\dots,F_m\}$ such that all pairs $(P_i,P_j)$ of pixels from the same year with the same field ID, it holds that $P_i\in F_k \iff P_j\in F_k$.}. 
        Nevertheless, we claim, that our results are not corrupted by the correlation of neighboring pixels. This is because our result is not a 'good' YPE, but the selected IS. So all tested ISs benefit equally from this correlation in terms of YPE and we are only interested in the relative differences.

    }

    \subsection{NDVI Correction as Unsupervised Learning}
        The question arises if we can build the correction model on the same year as we want to apply it on. Usually, a similar approach might carry the danger of overfitting. However, we have not used any ground truth at any point (until the evaluation). Instead, we estimated the ``true'' NDVI with the assumption~\ref{true_ndvi_assumption} via OOB. In other words, we have not used any ground truth but rather developed an unsupervised learner of the NDVI. Consequently, we reason that we can apply our method to a new (comparable) dataset.
    \subsection{Using Additional Covariates}{
        In section~\ref{sec:corr_data_table} we have only used the spectral data (and the observational NDVI calculated from them) as covariates. Since we have the weather data available (cf. REF-SEC), it would be a small effort to incorporate it, together with statistics collected from it (i.e., GDD or `rainfall in the last 30 days'). 

        We decided against using this data, because on the one hand we have the problem that we have practically too few observations (we observe only 5 years) and we expect the weather in our study region to be rather homogeneous which is suggested by the fact that the weather data published by Meteoswiss are for a grid with a resolution of 1 km. On the other hand, we want the underlying model not to learn improper relationships. For example, the model might automatically predict a high NDVI for a day in summer (detected by high GDD / many sunshine hours / high temperature) just because it is ``used'' to observing a lot of vegetation in summer. 
        Including temporally (e.g., $P_{t-1}$ and $P_{t+1}$) and geographically adjacent pixels would likely improve performance. However, for simplicity, we omit it here\footnote{This is done for simplicity of understanding and using the model, since one would need to adapt to some convention of how to supply the data of adjacent pixels without redundancy (i.e., supplying $P_t$ multiple times).}.
}
}

% Comment from Gregor:
    % {You already capture the "main" structure of your thesis with the interpolation and the NDVi correction sections. Can you combine them both in a "synthesis" subsection at the end of the discussion?}
        % --> NO, synthesis is given in Conclusion
