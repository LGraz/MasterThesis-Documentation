\chapter{Discussion}\label{sec:discussion}
    In the first part of the discussion, we examine IMs for compatibility with data gaps and argue choices for selected IMs. In the second part, we identify the best IS and discuss issues that have arisen in the context of the NDVI correction.


\section{{{IM}}s}{ \label{sec:discussion_itpl}
    \subsection{Data Gaps in Time Series}\label{sec:discussion_itpl_data_gaps}{
        NW estimates the value for $t$ by relating to the points near $t$. To determine what `near' means, a bandwidth $h$ is used (cf. equation~\refeq{eq:kernel_with_bandwidt}). This gets problematic as soon as the data gaps become larger than $h$, since no points are left that are close to $t$. 

        Regarding the GK, we expect that due to the stationarity assumption, the interpolation will always tend to the mean if data gaps are present (cf. figure~\ref{fig:kriging_parameters}). 

        Since the SG requires equidistant points, data gaps will break it. \cite{chenSimpleMethodReconstructing2004a} proposes a linear interpolation to restore missing data points. However, due to the timescale transformation to GDD in section~\ref{sec:gdd_def}, the requirement of equidistant remains an unresolved issue.

        % FS SS LOESS DL BSPL 
        We do not trust the FS interpolation if there are noticeable data gaps. As could be observed in figure~\ref{fig:interpol/fourier_dl_comparison} the curve can escape there. Additionally, the poor goodness-of-fit values for the robustified variant in table~\ref{tab:cv-statistics_itpl-methods} illustrate the unreliability of this IM method.
        These are meaningful in describing the ability to cope with data gaps, since more data points are ignored during the robustification and thus data gaps are simulated. 

        Similarly, for SS, LOESS, DL, and BS we compare the values in table~\ref{tab:cv-statistics_itpl-methods} between the robustified and non-robust variant. We find that the robust variant does not differ strongly from the non-robust variant (unlike as for FS). Thus, we conclude that these methods do not have systematic failures.

        Regarding the LOESS, in case of data gaps, the weights can attain non-intuitive values. The result can be a strongly fluctuating behavior, as observed in figure~\ref{fig:interpol/2x3_loess_robust} in plot (c). There, a strange peak between the first and second observation is visible. This peak originates from local weighting. In this case, the first data point in the plot, although adjacent to the peak, is given a low weight compared to the points to the right of the peak (for estimating the value at this peak).

        In our experience, the DL handles data gaps well, but it may happen that the model describes the NDVI increase as abrupt. This, however, was fixed, by bounding the first derivative (cf. section~\ref{sec:itpl_param_optimizationissues}).
    }

    \subsection{Preselection}{\label{sec:itpl_preselection}
        Here we justify our preselection of the {{IM}}s tested in section~\ref{sec:itpl_perfomance_assessment}. 
        We decided against NW Because of its systematic errors at peaks and valleys. Moreover, this method handles data gaps poorly (cf. section~\ref{sec:discussion_itpl_data_gaps}). 
        Moreover, UK will not be considered since the underlying stationarity assumption is not met and therefore a systematic bias is introduced. On top of that, maximum likelihood parameter estimation occasionally might lead to overfitting (cf. figure~\ref{fig:kriging_parameters}).
        Also, we do not include the SG in the next selection, since we see it as a special case of LOESS.
        The remaining IMs are thus SS LOESS DL BS and FS.
    }

    \subsection{Candidate Selection}{\label{sec:itpl_candiate_selection}
        Given that DL convinces regarding most of the selected score functions in table~\ref{tab:cv-statistics_itpl-methods} we will apply this method also in chapter~\ref{sec:corr}. Moreover, we see that the robustification mostly improved the score regarding QAR\textsuperscript{50}, QAR\textsuperscript{75}, QAR\textsuperscript{85} and QAR\textsuperscript{90}. Only for the outlier-sensitive score functions (RMSE and QAR\textsuperscript{95})\footnote{For the RMSE one outlier is enough to take away the usefulness of the statics, in the case of QAR\textsuperscript{95} it is enough if 5\% of the data are contaminated to break the statics.} we notice significant worsening (we consider the robust FS separately in section~\ref{sec:discussion_itpl_data_gaps}). Consequently, we will also use the robustification in section~\ref{sec:corr}.
        In order to not only rely on the form assumptions of the DL, we further choose a non-parametric method for further consideration. Despite the LOESS slightly dominating the SS in table~\ref{tab:cv-statistics_itpl-methods}, we choose the SS. This is due to the strange behavior of the LOESS in case of data gaps (see section~\ref{sec:discussion_itpl_data_gaps}) and the good interpretability of the SS using the minimization function~\refeq{eq:ss}.
    }
}


\section{NDVI Correction}{\label{sec:discussion_corr}
    \subsection{Choose {{IS}}}\label{sec:discussion_iplfstrategy-choose}

    The evaluation of various ISs via the YPE (cf. section~\ref{sec:results_ndvi_corr}) shows that SS are better suited than DL for yield estimation. Moreover, it seems surprising that robustification tends to worsen the results, despite reducing LOOCV residuals in most cases (cf. section~\ref{sec:results_itpl}). We conjecture that the correction models handle outliers by themselves (by correcting or down-weighting them) and thus do not benefit from an external robustification. Indeed, for OLS\textsuperscript{SCL} we see in equation~\refeq{eq:corr_lm_res} that the smaller the observed NDVI of a point, the larger the estimated residual --- yielding a lower weight. This is consistent with our experience that outliers usually underestimate the NDVI. Our conjecture is consistent with the fact that if we do not correct, robustification produces a marginal improvement. 
    
    Using the best IS with correction (SS+OLS\textsuperscript{SCL}), instead of the best IS without correction (SS\textsuperscript{rob}), we can additionally explain $(0.148-0.140)/0.148 = 5.4\%$ of the variance in yield prediction (cf. table~\ref{tab:methods_vs_yieldprediction_relative}. To give a context, 100\% would allow us to model the yield perfectly.
    
    Note that the results discussed here depend strongly on the link function used (cf. equation~\refeq{eq:corr_link}). Once we change it, we should also repeat this analysis.
            


    \subsection{Investigation of Error Sources in Yield Estimation}{\label{sec:discuss_high-rmse-in-yield-prdiction}
        Although the YPE was not our primary goal but was only used as a means to select the best IS, we compare our values with the corresponding ones by \cite{perichPixelbasedCropYield2022}. There, a YPE 1.00 [t/ha] was obtained using weather data in addition to NDVI TS. Since our error is only about 3.3\% larger (cf. table~\ref{tab:methods_vs_yieldprediction}), we consider our results to be competitive. Especially as we did not use meteorological data aside from the timescale transformation (cf. section~\ref{sec:gdd_def}) and in contrary did not scale the yield down by 10\% (cf. section~\ref{sec:yieldmapping_data}. In the following, we ask ourselves how much modelling performance we can actually expect. This will be limited by multiple sources of uncertainty in the data:
        \begin{Nenumerate}
            \item Uncertainty in yield data collected by the combine harvester \citep{robinsonComparingPerformanceTechniques2005}.
            \item Uncertainty in yield data through rasterization.
            \item Uncertainty in satellite images through `measurement errors' introduced via clouds and other atmospheric effects (cf. section~\ref{sec:s2_challangges}).
            \item Heterogeneity within one pixel that includes, for example, very dense vegetation (and thus according to \cite{guNDVISaturationAdjustment2013} saturation of the NDVI) on the one half and dry soil at the other half leads to a less informative NDVI value.
            \item Uncertainty introduced by interpolating NDVI TS (especially when long data-gaps are present).
        \end{Nenumerate}
        \todo[inline]{find citations for the above}
        Furthermore, even if we would have a perfect NDVI curve, it contains only a fraction of the information about the underlying vegetation. 
        Nonetheless, \cite{perichPixelbasedCropYield2022} manages to explain up to 86\% of the variance in crop yield with only the NDVI TS and weather data (Table 5).  Although the authors divided the data into training and test data, this subdivision was done randomly at pixel level (without subdividing into fields or years). Thus, there are pixels in the training data that are neighboring pixels from the test data and consequently exhibit high correlations (in yield and NDVI). We suspect that overfitting via high-correlation pixels is responsible for these high values. On the other hand, the authors observe poor results for cross-year-validation\footnote{By cross-year-validation we understand a cross validation with respect to the RMSE, where each year represents a single fold.} (table 6) and account them to uneven (extreme) weather.  If this is not rather caused by the suspected overfitting, could be investigated by performing a cross-field-validation\footnote{By cross-field-validation we understand the same as with cross-year-validation but with splitting each fold (i.e., a year) further into the respective fields. Since we have multiple fields per year, during evaluation each model trained will have seen the weather of all years but no adjacent pixels.}.%\footnote{By cross-field-validation we understand a cross validation with respect to the RMSE with a partitioning $\mathcal{F}=\{F_1,\dots,F_m\}$ such that all pairs of pixels from the same year with the same field ID, it holds that both pixels are in the same $F_k$.}.
        Nevertheless, we claim, that our results are affected by spatial correlation of neighboring pixels. This is because we expect all tested ISs to benefit equally from this correlation in terms of YPE, and we are only interested in the relative differences. our result is not a 'good' YPE, but the selected IS. So 
    }

    \subsection{NDVI Correction as Unsupervised Learning}
        The question arises if we can build the correction model on the same year as we want to apply it on. Usually, a similar approach might carry the danger of overfitting. However, we have not used any ground truth at any point (until the evaluation). Instead, we estimated the `true' NDVI with the assumption~\ref{true_ndvi_assumption} via OOB. In other words, we have not used any ground truth but rather developed an unsupervised learner of the NDVI. Consequently, we reason that we can apply our method to a new (comparable) dataset.
    \subsection{Using Additional Covariates}{
        In section~\ref{sec:corr_data_table} we have only used covariates derived from spectral data. 
        We decided against using meteorological data, since we consider five years of data not to be sufficient to model patter of how vegetation reacts to various weather events. Moreover, we expect the weather in our study region to be rather homogeneous, which is suggested by the fact that the weather data published by Meteoswiss are for a grid with a resolution of 1 km. On the other hand, we want the underlying model not to learn improper relationships. For example, the model might automatically predict a high NDVI for a day in summer (detected by high GDD or many sunshine hours) just because it is `used' to observing a lot of vegetation in summer. 
        Including temporally (e.g., $P_{t-1}$ and $P_{t+1}$) and geographically adjacent pixels would likely improve performance. However, for simplicity, we omit it here\footnote{This is done for simplicity of understanding and using the model, since one would need to adapt to some convention of how to supply the data of adjacent pixels without redundancy (i.e., supplying $P_t$ multiple times). Another complication is a border-pixel with some adjacent pixels outside the field.}.
    }
}

% Comment from Gregor:
    % {You already capture the "main" structure of your thesis with the interpolation and the NDVi correction sections. Can you combine them both in a "synthesis" subsection at the end of the discussion?}
        % --> NO; synthesis is given in Conclusion
