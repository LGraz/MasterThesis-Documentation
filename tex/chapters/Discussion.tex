\chapter{Discussion}\label{sec:discussion}
    In the first part of the discussion, we argue the choice of the score function, examine IMs for compatibility with data gaps and argue choices for selected IMs. In the second part, we argue against using meteorological data, identify the best IS and address issues in yield estimation.


\section{Interpolation}{ \label{sec:discussion_itpl}
    \subsection{Score Functions}{
        In situ data is generally preferred for evaluating IMs. Being difficult to obtain, \cite{liHighqualityVegetationIndex2021} generated NDVI TS by introducing random noise to an idealized NDVI curve that has been constructed by considering many pixeles and taking the pointwise mean of their NDVI TS. Because the distribution of the noise is chosen by the authors, they do not have to deal with systematic outliers and adequately, given the circumstances, use the non-robust RMSE. On the other hand, the generated NDVI TS no longer contain the typical challenges (underestimated NDVI and data gaps). Thus, the authors test their IMs more for general interpolation reliability, and less for challenging S2-derived NDVI TS. To adequately test our IMs for those challenges, we employ the LOOCV.

        \cite{caiPerformanceSmoothingMethods2017a} evaluate various IMs based solely on the RMSE. We see the choice of the RMSE for the selection of the IMs as problematic, because it is sensitive to outliers. Yet we know that systematic outliers are present in our data (cf. figure~\ref{fig:satelite/time_series_2021_P112/35_scl4_2021-06-03.png}). Thus, when we consider the RMSE as a score function, we do not choose the IM that disregards outliers, but is heavily influenced by them. Likewise, \cite{liemohnRMSENotEnough2021} confirms that the RMSE, in the presence of outliers, is not sufficient. \cite{barronGeneralAdaptiveRobust2019} proposes a robust score function, a more flexible variant of the Huber loss function \citep{huberRobustEstimationLocation1964}. For simplicity's sake, we choose the robust QAR\textsuperscript{x} score function (cf. definition~\ref{def:qar}). For example, QAR\textsuperscript{90} can easily handle up to 10\% of outliers (i.e., contaminated data points) in the data.                 
    }

    \subsection{Data Gaps in Time Series}\label{sec:discussion_itpl_data_gaps}{
        NW estimates the value for the time $t$ by relating to the points near $t$. To determine what `near' means, a bandwidth $h$ is used (cf. equation~\refeq{eq:kernel_with_bandwidt}). This approach becomes infeasible as soon as the data gaps become larger than $h$, since no points are left that are close to $t$. Using a locally adaptive bandwidth fix \citep{brockmannLocallyAdaptiveBandwidth1993}, we pay with a greater variability of the estimator. According to the authors, a small sample size (as we have it for the NDVI TS) worsens this situation. 
%%opt%%
        For GK, we expect, due to the stationarity assumption, that in reagions with no data the interpolation will tend to the mean (cf. figure~\ref{fig:kriging_parameters}). 
%%opt%%
        The SG cannot handle data gaps since it requires equidistant data points. \cite{chenSimpleMethodReconstructing2004a} proposes a linear interpolation to restore missing data points. However, due to the non-linear timescale transformation to GDD in section~\ref{sec:gdd_def}, the requirement of equidistant remains an unresolved issue.
%%opt%%
        % FS SS LOESS DL BSPL 
        The FS interpolation can be corrupted if there are noticeable data gaps, as can be observed in figure~\ref{fig:interpol/fourier_dl_comparison}.  Additionally, the poor goodness-of-fit values for the robustified variant in table~\ref{tab:cv-statistics_itpl-methods} illustrate the unreliability of this IM method.
        The robustified values in table~\ref{tab:cv-statistics_itpl-methods} are meaningful in describing the ability to cope with data gaps, since more data points are ignored during the robustification and hence data gaps are simulated. 
%%opt%%
        Similarly, for SS, LOESS, DL, and BS we compare the values in table~\ref{tab:cv-statistics_itpl-methods} between the robustified and non-robust variant. Unlike as for FS, the robust variant does not differ significantly from the non-robust variant. Thus, we conclude that these methods do not exhibit systematic failures frequently.
%%opt%%
        Regarding the LOESS, the weights can attain non-intuitive values in case of data gaps. The result can be a strongly fluctuating behavior, as observed in figure~\ref{fig:interpol/2x3_loess_robust} in plot (c). There, a strange peak between the first and second observation is visible. This peak originates from local weighting. In estimating the peak value, the first data point in the plot, although adjacent to the peak, is given a low weight compared to the points to the right of the peak.
%%opt%%
        Due to strong shape assumptions, DL handles data gaps well. However, it may happen that the model describes the NDVI increase as abrupt. This, however, was fixed by bounding the first derivative (cf. section~\ref{sec:itpl_param_optimizationissues}).
    }

    \subsection{Preselection}{\label{sec:itpl_preselection}
        Here we justify our preselection of the {{IM}}s tested in section~\ref{sec:itpl_perfomance_assessment}. 
        We decided against NW because of its systematic errors at peaks and valleys and poor handling of data gaps (cf. section~\ref{sec:discussion_itpl_data_gaps}). 
        UK will not be considered since the underlying stationarity assumption is not met and therefore a systematic bias is introduced. On top of that, maximum likelihood parameter estimation might lead to overfitting (cf. figure~\ref{fig:kriging_parameters}).
        Also, we do not include SG, since we see LOESS as a more favourable generalization.
        The remaining IMs are thus SS, LOESS, DL, BS, and FS.
    }

    \subsection{Candidate Selection}{\label{sec:itpl_candiate_selection}
        At this point we choose which IMs will be considered in chapter~\ref{sec:corr}.
        Given that DL convinces, regarding most of the selected score functions in table~\ref{tab:cv-statistics_itpl-methods}, we will further consider this IM. Furthermore, we see that in most cases the robustification improved the score regarding QAR\textsuperscript{50}, QAR\textsuperscript{75}, QAR\textsuperscript{85} and QAR\textsuperscript{90}. Only for the outlier-sensitive score functions (RMSE and QAR\textsuperscript{95})\footnote{For the RMSE one outlier is enough to take away the usefulness of the statics, in the case of QAR\textsuperscript{95} it is enough if 5\% of the data are contaminated to break the statics.} we notice significant worsening (we consider the robust FS separately in section~\ref{sec:discussion_itpl_data_gaps}). Consequently, we will also apply the robustification in chapter~\ref{sec:corr}.
        In order to not only rely on the shape assumptions of the DL, we additionally choose a flexible non-parametric IM for further consideration. Despite the LOESS slightly dominating the SS in table~\ref{tab:cv-statistics_itpl-methods}, we choose the SS. We justify this selection with the non-smooth behavior of the LOESS in case of data gaps (see section~\ref{sec:discussion_itpl_data_gaps}) and the good interpretability of the SS from minimizing function~\refeq{eq:ss}.
    }
}


\section{NDVI Correction}{\label{sec:discussion_corr}
  
    \subsection{Using Additional Covariates}{
        In section~\ref{sec:corr_data_table} we have only used covariates derived from spectral data. 
        We decided against using meteorological data, since we consider five years of data not to be sufficient to model patterns of how vegetation reacts to various weather events. Moreover, we expect the weather in our study region to be rather homogeneous, which is suggested by the fact that the meteorological data published by Meteoswiss are for a grid with a resolution of 1 km. On the other hand, we want the underlying model not to learn improper relationships. For example, the model might automatically predict a high NDVI for a day in summer (detected by high GDD or many sunshine hours) just because it is `used' to observing a lot of vegetation in summer. 
        Including temporally (e.g., $P_{t-1}$ and $P_{t+1}$) and geographically adjacent pixels would likely improve performance. However, for simplicity, we omitted it here\footnote{This is done for simplicity of understanding and using the model, since one would need to adapt to some convention of how to supply the data of adjacent pixels without redundancy (i.e., supplying $P_t$ multiple times). Another complication would be a border-pixel with some adjacent pixels outside the field.}.
    }

    \subsection{{{IS}} Selection}\label{sec:discussion_iplfstrategy-choose}

    The evaluation of various ISs via the YPE in section~\ref{sec:results_ndvi_corr} shows that SS are better suited than DL for yield estimation. Moreover, it seems surprising that robustification tends to worsen the results, despite reducing LOOCV residuals in most cases (cf. section~\ref{sec:results_itpl}). We conjecture that the correction models handle outliers by themselves (by correcting or down-weighting them) and thus do not benefit from an external robustification. Indeed, for OLS\textsuperscript{SCL} we see in equation~\refeq{eq:corr_lm_res} that the smaller the observed NDVI of a point, the larger the estimated residual --- yielding a lower weight. This coincides with our experience that outliers usually underestimate the NDVI. Our conjecture is consistent with the fact that if we do not correct, robustification produces a marginal improvement. 
    
    If we use the best IS without correction (SS\textsuperscript{rob}) the relative RMSE of the NDVI-based yield prediction is 0.148 (cf. table~\ref{tab:methods_vs_yieldprediction_relative}). Using the best IS with correction (SS+OLS\textsuperscript{SCL}), instead results in a relative RMSE of 0.140. Later, in section~\ref{sec:discuss_high-rmse-in-yield-prdiction}, we explain why those results might be too optimistic but argue that they are still valid for relative comparison. Hence, we compare the amount of unexplained variance of the NDVI-based yield prediction. When correcting with SS+OLS\textsuperscript{SCL} instead of only using SS\textsuperscript{rob}, the unexplained variance decreases by 
    $$\frac{0.148^2-0.140^2}{0.148^2} = 10.5\%.\footnote{The same result may be obtained via the $R^2$ values from appendix table~\ref{tab:methods_vs_yieldprediction_r2}: $\frac{(1-0.705)  -  (1-0.736)}{ 1-0.705}   = 10.5\%$}$$
    Note that the results discussed here depend strongly on the link function used (cf. equation~\refeq{eq:corr_link}). Once we change it, we should also repeat this analysis.
            


    \subsection{Error Sources in Yield Estimation}{\label{sec:discuss_high-rmse-in-yield-prdiction}
        Although the YPE was not our primary goal, but was only used as a means to select the best IS, we compare our values with the corresponding ones by \cite{perichPixelbasedCropYield2022}. There, a YPE 1.00 [t/ha] was obtained using meteorological data in addition to NDVI TS. Since our error is only about 3.3\% larger (cf. table~\ref{tab:methods_vs_yieldprediction}), we consider our results to be competitive. Especially as we did not use meteorological data aside from the timescale transformation (cf. section~\ref{sec:gdd_def}), and in contrary did not scale the yield down by 10\% (cf. section~\ref{sec:yieldmapping_data}). In the following, we ask ourselves how much modelling performance we can expect. This will be limited by multiple sources of uncertainty in the data:
        \begin{Nenumerate}
            \item Uncertainty in yield data collected by the combine harvester \citep{robinsonComparingPerformanceTechniques2005}.
            \item Uncertainty in yield data through rasterization.
            \item Contamination of satellite images through clouds and other atmospheric effects (cf. section~\ref{sec:s2_challangges}).
            \item Heterogeneity within one pixel might lead to a less informative NDVI value.\footnote{For example, if one pixel is coverd densly with vegetation on one half while the other half is covered with dry soil, the mean reflectancy does not correspond to the mean vegetation density. This is caused by the fast saturation of the NDVI \citep{guNDVISaturationAdjustment2013}.} 
            \item Uncertainty introduced by interpolating NDVI TS, especially when long data-gaps are present.
        \end{Nenumerate}
        Even if we had a perfect NDVI curve, it contains only a fraction of the information about the underlying vegetation. 
        Nonetheless, \cite{perichPixelbasedCropYield2022} manages to explain up to 86\% of the variance in crop yield with only the NDVI TS and meteorological data (Table 5).  Although the authors divided the data into training and test data, this subdivision was done randomly at pixel level (without subdividing into fields or years). Thus, there are pixels in the training data that are neighboring pixels from the test data and consequently exhibit high correlations (in yield and spectral reflectances). We suspect that these high values are due to overfitting via high-correlation pixels. This line of argument is consistent with the poor results in table~6 for cross-year-validation --- i.e., each year represents a single fold. The authors, however, account them to uneven (extreme) weather.  If this is not rather caused by the suspected overfitting, could be investigated by performing a cross-field-validation --- i.e., cross-year-validation with splitting each year further into the respective fields. Since we have multiple fields per year, during evaluation each model trained will have seen the weather of all years but no adjacent pixels. %\footnote{By cross-field-validation we understand a cross validation with respect to the RMSE with a partitioning $\mathcal{F}=\{F_1,\dots,F_m\}$ such that all pairs of pixels from the same year with the same field ID, it holds that both pixels are in the same $F_k$.}.
        Nevertheless, we claim, that our results are not affected by spatial correlation of neighboring pixels. This is because our result is not a 'good' YPE, but the selected IS. Furthermore, we expect all tested ISs to benefit equally from this correlation in terms of YPE, and we are only interested in the relative differences.  
    }

    \subsection{NDVI Correction as Unsupervised Learning}
        The question arises, if we can build the correction model on the same year as we want to apply it on. Usually, a similar approach might carry the danger of overfitting. However, we have not used any ground truth at any point (until the evaluation). Instead, we estimated the `true' NDVI with the assumption~\ref{true_ndvi_assumption} via OOB. Hence, we developed an unsupervised learner of the NDVI. The degree of overfitting can be objectively assessed with the YPE. Using the IS corresponding to the best YPE, we do not expect strong overfitting. Consequently, we reason that our method can be applied to a new comparable dataset.
}

% Comment from Gregor:
    % {You already capture the "main" structure of your thesis with the interpolation and the NDVI correction sections. Can you combine them both in a "synthesis" subsection at the end of the discussion?}
        % --> NO; synthesis is given in Conclusion
