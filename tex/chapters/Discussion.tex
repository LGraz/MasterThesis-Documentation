\chapter{Discussion}\label{sec:discussion}
    In the first part of the discussion, we examine IMs for compatibility with data gaps, argue choices for selected IMs and discuss the choice of the score function. In the second part, we identify the best IS and discuss issues that have arisen in the context of the NDVI correction.


\section{Interpolation}{ \label{sec:discussion_itpl}
    \subsection{Data Gaps in Time Series}\label{sec:discussion_itpl_data_gaps}{
        NW estimates the value for the time $t$ by relating to the points near $t$. To determine what `near' means, a bandwidth $h$ is used (cf. equation~\refeq{eq:kernel_with_bandwidt}). This approach becomes problematic as soon as the data gaps become larger than $h$, since no points are left that are close to $t$. Using a locally adaptive bandwidth fix \citep{brockmannLocallyAdaptiveBandwidth1993}, we pay with a greater variability of the estimator. According to the authors, a small sample size (as we have it for the NDVI TS) worsens this situation. 

        Regarding the GK, we expect that due to the stationarity assumption, the interpolation will always tend to the mean if data gaps are present (cf. figure~\ref{fig:kriging_parameters}). 

        Since the SG requires equidistant points, data gaps will break it. \cite{chenSimpleMethodReconstructing2004a} proposes a linear interpolation to restore missing data points. However, due to the timescale transformation to GDD in section~\ref{sec:gdd_def}, the requirement of equidistant remains an unresolved issue.

        % FS SS LOESS DL BSPL 
        The FS interpolation can be corrupted if there are noticeable data gaps, as can be observed in figure~\ref{fig:interpol/fourier_dl_comparison}.  Additionally, the poor goodness-of-fit values for the robustified variant in table~\ref{tab:cv-statistics_itpl-methods} illustrate the unreliability of this IM method.
        The values in \ref{tab:cv-statistics_itpl-methods} are meaningful in describing the ability to cope with data gaps, since more data points are ignored during the robustification and thus data gaps are simulated. 

        Similarly, for SS, LOESS, DL, and BS we compare the values in table~\ref{tab:cv-statistics_itpl-methods} between the robustified and non-robust variant. We find that the robust variant does not differ significantly from the non-robust variant (unlike as for FS). Thus, we conclude that these methods do not have systematic failures.

        Regarding the LOESS, in case of data gaps, the weights can attain non-intuitive values. The result can be a strongly fluctuating behavior, as observed in figure~\ref{fig:interpol/2x3_loess_robust} in plot (c). There, a strange peak between the first and second observation is visible. This peak originates from local weighting. In this case, the first data point in the plot, although adjacent to the peak, is given a low weight compared to the points to the right of the peak (for estimating the value at this peak).

        In our experience, the DL handles data gaps well due to strong shape assumptions (no jumps), but it may happen that the model describes the NDVI increase as abrupt. This, however, was fixed by bounding the first derivative (cf. section~\ref{sec:itpl_param_optimizationissues}).
    }

    \subsection{Preselection}{\label{sec:itpl_preselection}
        Here we justify our preselection of the {{IM}}s tested in section~\ref{sec:itpl_perfomance_assessment}. 
        We decided against NW because of its systematic errors at peaks and valleys and poor handling of data gaps (cf. section~\ref{sec:discussion_itpl_data_gaps}). 
        The UK will not be considered since the underlying stationarity assumption is not met and therefore a systematic bias is introduced. On top of that, maximum likelihood parameter estimation occasionally might lead to overfitting (cf. figure~\ref{fig:kriging_parameters}).
        Also, we do not include the SG in the next selection, since we see it as a special case of LOESS.
        The remaining IMs are thus SS, LOESS, DL, BS, and FS.
    }

    \subsection{Candidate Selection}{\label{sec:itpl_candiate_selection}
        Given that DL convinces, regarding most of the selected score functions in table~\ref{tab:cv-statistics_itpl-methods}, we will apply this method also in chapter~\ref{sec:corr}. Moreover, we see that the robustification in most cases improved the score regarding QAR\textsuperscript{50}, QAR\textsuperscript{75}, QAR\textsuperscript{85} and QAR\textsuperscript{90}. Only for the outlier-sensitive score functions (RMSE and QAR\textsuperscript{95})\footnote{For the RMSE one outlier is enough to take away the usefulness of the statics, in the case of QAR\textsuperscript{95} it is enough if 5\% of the data are contaminated to break the statics.} we notice significant worsening (we consider the robust FS separately in section~\ref{sec:discussion_itpl_data_gaps}). Consequently, we will also use the robustification in section~\ref{sec:corr}.
        In order to not only rely on the form assumptions of the DL, we further choose a non-parametric method for further consideration. Despite the LOESS slightly dominating the SS in table~\ref{tab:cv-statistics_itpl-methods}, we choose the SS. We justify this selection with the non-smooth behavior of the LOESS in case of data gaps (see section~\ref{sec:discussion_itpl_data_gaps}) and the good interpretability of the SS from minimizing function~\refeq{eq:ss}.
    }

    \subsection{Score Functions}{
        In situ data generally prefered for evaluating various IMs. Being difficult to obtain, \cite{liHighqualityVegetationIndex2021} generated NDVI TS by introducing random roise to an idealized NDVI curve that has been constructed by taking the mean of multiple NDVI TS of several years. Because the distribution of the noise is known, the authors do not have to deal with systematic outliers and adequately, given the circumstances, use the non-robust RMSE. On the other hand, the generated NDVI TS no longer contain the typical challenges (underestimated NDVI and data gaps). Thus, the authors test their IMs more for general interpolation reliability, and less for challenging S2-derived NDVI TS. To adequatly test our IMs for those challenges, we employ the LOOCV.
        \cite{caiPerformanceSmoothingMethods2017a} evaluate various IMs based solely on the RMSE. We see the choice of the RMSE for the selection of the IMs as problematic, because it is sensitive to outliers. Yet we know that systematic outliers are present in our data (cf. figure~\ref{fig:satelite/time_series_2021_P112/35_scl4_2021-06-03.png}). Thus, when we consider the RMSE as a score function, we do not choose the IM that disregards outliers, but is heavily influenced by them. Likewise, \cite{liemohnRMSENotEnough2021} confirms that the RMSE, in the presence of outliers, is not sufficient. \cite{barronGeneralAdaptiveRobust2019} proposes a robust score function, a more flexible variant of the huber loss function \citep{huberRobustEstimationLocation1964}. For simplicity's sake, we choose the robust QAR\textsuperscript{x} score function (cf. definition~\ref{def:qar}). For example, QAR\textsuperscript{90} can easily handle up to 10\% of outliers (i.e., contaminated data points) in the data.                 
    }
}


\section{NDVI Correction}{\label{sec:discussion_corr}
    \subsection{{{IS}} Selection}\label{sec:discussion_iplfstrategy-choose}

    The evaluation of various ISs via the YPE (cf. section~\ref{sec:results_ndvi_corr}) shows that SS are better suited than DL for yield estimation. Moreover, it seems surprising that robustification tends to worsen the results, despite reducing LOOCV residuals in most cases (cf. section~\ref{sec:results_itpl}). We conjecture that the correction models handle outliers by themselves (by correcting or down-weighting them) and thus do not benefit from an external robustification. Indeed, for OLS\textsuperscript{SCL} we see in equation~\refeq{eq:corr_lm_res} that the smaller the observed NDVI of a point, the larger the estimated residual --- yielding a lower weight. This is consistent with our experience that outliers usually underestimate the NDVI. Our conjecture is consistent with the fact that if we do not correct, robustification produces a marginal improvement. 
    
    If we use the best IS without correction (SS\textsuperscript{rob}) the relative RMSE of the NDVI-based yield prediction is 0.148 (cf. table~\ref{tab:methods_vs_yieldprediction_relative}). Using the best IS with correction (SS+OLS\textsuperscript{SCL}), instead results in a relative RMSE of 0.140. Later, in section~\ref{sec:discuss_high-rmse-in-yield-prdiction}, we explain why those results might be too optimistic but argue that they are still valid for relative comparison. Hence, we compare the amount of unexplained variance of the NDVI-based yield prediction. That is $1-R^2$ (for $R^2$ values, see appendix table~\ref{tab:methods_vs_yieldprediction_r2}). Consequently, when correcting our NDVI TS, the unexplained variance decreases by:
    $$
        \frac{(1-0.705)  -  (1-0.736)}{ 1-0.705}   = 10.5\%\footnote{The calculation could be also done using the relative RMSE values from table~\ref{tab:methods_vs_yieldprediction_relative} via: $(0.148^2-0.140^2)/0.148^2$.}
    $$
    Note that the results discussed here depend strongly on the link function used (cf. equation~\refeq{eq:corr_link}). Once we change it, we should also repeat this analysis.
            


    \subsection{Investigation of Error Sources in Yield Estimation}{\label{sec:discuss_high-rmse-in-yield-prdiction}
        Although the YPE was not our primary goal, but was only used as a means to select the best IS, we compare our values with the corresponding ones by \cite{perichPixelbasedCropYield2022}. There, a YPE 1.00 [t/ha] was obtained using meteorological data in addition to NDVI TS. Since our error is only about 3.3\% larger (cf. table~\ref{tab:methods_vs_yieldprediction}), we consider our results to be competitive. Especially as we did not use meteorological data aside from the timescale transformation (cf. section~\ref{sec:gdd_def}), and in contrary did not scale the yield down by 10\% (cf. section~\ref{sec:yieldmapping_data}). In the following, we ask ourselves how much modelling performance we can actually expect. This will be limited by multiple sources of uncertainty in the data:
        \begin{Nenumerate}
            \item Uncertainty in yield data collected by the combine harvester \citep{robinsonComparingPerformanceTechniques2005}.
            \item Uncertainty in yield data through rasterization.
            \item Contamination of satellite images through clouds and other atmospheric effects (cf. section~\ref{sec:s2_challangges}).
            \item Heterogeneity within one pixel that includes, for example, very dense vegetation (and thus according to \citeauthor{guNDVISaturationAdjustment2013} saturation of the NDVI) on the one half and dry soil at the other half leads to a less informative NDVI value.
            \item Uncertainty introduced by interpolating NDVI TS, especially when long data-gaps are present.
        \end{Nenumerate}
        Even if we had a perfect NDVI curve, it contains only a fraction of the information about the underlying vegetation. 
        Nonetheless, \cite{perichPixelbasedCropYield2022} manages to explain up to 86\% of the variance in crop yield with only the NDVI TS and meteorological data (Table 5).  Although the authors divided the data into training and test data, this subdivision was done randomly at pixel level (without subdividing into fields or years). Thus, there are pixels in the training data that are neighboring pixels from the test data and consequently exhibit high correlations (in yield and spectral reflectances). We suspect that these high values are due to overfitting via high-correlation pixels. This line of argument is consistent with the poor results for cross-year-validation\footnote{By cross-year-validation we understand a cross validation with respect to the RMSE, where each year represents a single fold.} (table 6). The authors, however, account them to uneven (extreme) weather.  If this is not rather caused by the suspected overfitting, could be investigated by performing a cross-field-validation\footnote{By cross-field-validation we understand the same as with cross-year-validation but with splitting each fold (i.e., a year) further into the respective fields. Since we have multiple fields per year, during evaluation each model trained will have seen the weather of all years but no adjacent pixels.}.%\footnote{By cross-field-validation we understand a cross validation with respect to the RMSE with a partitioning $\mathcal{F}=\{F_1,\dots,F_m\}$ such that all pairs of pixels from the same year with the same field ID, it holds that both pixels are in the same $F_k$.}.
        Nevertheless, we claim, that our results are not affected by spatial correlation of neighboring pixels. This is because our result is not a 'good' YPE, but the selected IS. Furthermore, we expect all tested ISs to benefit equally from this correlation in terms of YPE, and we are only interested in the relative differences.  
    }

    \subsection{NDVI Correction as Unsupervised Learning}
        The question arises if we can build the correction model on the same year as we want to apply it on. Usually, a similar approach might carry the danger of overfitting. However, we have not used any ground truth at any point (until the evaluation). Instead, we estimated the `true' NDVI with the assumption~\ref{true_ndvi_assumption} via OOB. In other words, we have not used any ground truth but rather developed an unsupervised learner of the NDVI. Consequently, we reason that we can apply our method to a new (comparable) dataset.
    \subsection{Using Additional Covariates}{
        In section~\ref{sec:corr_data_table} we have only used covariates derived from spectral data. 
        We decided against using meteorological data, since we consider five years of data not to be sufficient to model patterns of how vegetation reacts to various weather events. Moreover, we expect the weather in our study region to be rather homogeneous, which is suggested by the fact that the meteorological data published by Meteoswiss are for a grid with a resolution of 1 km. On the other hand, we want the underlying model not to learn improper relationships. For example, the model might automatically predict a high NDVI for a day in summer (detected by high GDD or many sunshine hours) just because it is `used' to observing a lot of vegetation in summer. 
        Including temporally (e.g., $P_{t-1}$ and $P_{t+1}$) and geographically adjacent pixels would likely improve performance. However, for simplicity, we omitted it here\footnote{This is done for simplicity of understanding and using the model, since one would need to adapt to some convention of how to supply the data of adjacent pixels without redundancy (i.e., supplying $P_t$ multiple times). Another complication would be a border-pixel with some adjacent pixels outside the field.}.
    }
}

% Comment from Gregor:
    % {You already capture the "main" structure of your thesis with the interpolation and the NDVi correction sections. Can you combine them both in a "synthesis" subsection at the end of the discussion?}
        % --> NO; synthesis is given in Conclusion
