
%     ML-methoden: muss ich sie beschreiben? Es scheint mir wissen zu sein, was ich vorraussetzen kann. 
    % --> ja, kurz und bündig erklären
\subsection{Ordinary Least Squares}{\label{sec:corr_model_OLS}
    The Ordinary Least Squares estimator (OLS) is a linear model that aims to minimize the sum of the squared residuals. We assume a linear relationship between $y$ and $X$ and allow for Gaussian noise. That is:
    \begin{equation}
        \label{eq:ols}
        y = X\beta  + \epsilon \quad \text{ where }\epsilon \overset{i.i.d.}{\sim}\mathcal{N}(0,\sigma^2)
    \end{equation}
    Assuming that $(X^TX)$ is regular, we can estimate the regression coefficients $\beta$ by
    \begin{equation}
        \hat \beta = (X^TX)^{-1}X^Ty = \argmin_{\beta in \R^p}\|y - X\beta\|_2^2 
    \end{equation}

    We will train two models, one using all covariates discussed above and one using only the SCL-classes and the observed NDVI. 

    \begin{my_pros_cons_table}{
        \item Simple method with good interpretability of coefficients.
        \item Computationally cheap.
    }{
        \item Catches only linear relationships.
        \item No integrated variable selection.\footnote{There is the possibility of stepwise model selection by dropping or adding some covariates, but for higher p this gets too expensive since the time complexity grows in $\mathcal{O}(np^4)$ (computing $X^TX$ requires $\mathcal{O}(np^2)$ which assuming $n>p$ dominates $\mathcal{O}(p^3) $ needed for the Cholesky Decomposition of $X^TX$). % p^2 time OLS. 
        }
    }
    \end{my_pros_cons_table}
}
\subsection{Least Absolute Shrinkage and Selection Operator}{\label{sec:corr_model_LASSO}
    The Least Absolute Shrinkage and Selection Operator (LASSO) can be similarly expressed than the OLS but adds a penalty to the minimization problem:
    \begin{equation}
        \hat \beta_\lambda = \argmin_{\beta \in \R^p}\|y - X\beta\|_2^2 + \lambda \|\beta\|_1 = \argmin_{\beta \in \R^p \text{ and } \|\beta\|_1<\lambda}\|y - X\beta\|_2^2.\footnote{The last two terms are equivalent by lagrangian optimization.}
        \label{eq:lasso}
    \end{equation}
    Even though we do not have a closed form solution for equation \eqref{eq:lasso} we can solve it easily via optimization, since the function $\beta \in \{\beta\in\R^p|\|\beta\|_1<\lambda\}\|\ \mapsto \|y - X\beta\|_2^2$  is continuous and convex.

    \cite{tibshiraniRegressionShrinkageSelection2011} shows that the LASSO solution tends to be sparse. That is $\beta_i = 0$ for most $i = 1,\dots,p$. The larger $\lambda$, the more $\beta_i = 0$ and hence the simpler the resulting model.

    In order to know which $\lambda$ to choose, we try a large range of possible values. For each $\beta_\lambda$, we calculate the cross-validated $RMSE_\lambda$
    \footnote{The cross validated Root Mean Square Error is the mean of the RMSE's obtained for each fold using the model trained on the remaining folds. 
    %We use the following definition of the $RMSE$: $\sqrt{\sum_{i=1}^n(y-\hat y)^2/n}$
    } (and its standard deviation $\sigma_\lambda$ using the $k$ folds) and define the $\lambda$ with the smallest corresponding  $RMSE_\lambda$ as $\lambda_{min}$. From here we choose the largest $\lambda$ for which the $RMSE_\lambda$ is smaller than $RMSE_{\lambda_{min}}+\sigma_\lambda$. This yields a simpler model while keeping the $RMSE$ small.

    We will apply the LASSO using the selected covariates in section~\ref{sec:corr_data_table} and their second degree of interactions.\footnote{This is if our covariates are $\{1,a,b\}$, then we will now use $\{1,a,b,ab,a^2,b^2\}.$}
    
    \begin{my_pros_cons_table}{
        \item Usually yields a sparse solution. This tends to give better generalizability (prediction performance on unseen data).
        \item Successfully deals with correlation in covariates. 
        \item Interpretable results.
    }{
        \item Estimate is biased.
        \item Computationally expensive.
    }
    \end{my_pros_cons_table}
}

\subsection{General Additive Model}{\label{sec:corr_model_GAM}
    General Additive Models (GAM) as described in \cite{hastieGeneralizedAdditiveModels1987} are a special case of Projection Pursuit Regression, where only the $p$ directions parallel to the coordinate axes are considered. The result is different to a linear model since the coordinate functions are not restricted to be linear but are assumed to be non-parametric functions. The model can be written as:
    \begin{equation}
        \label{eq:gam}
        g_{add}(x) = \mu + \sum_{i=1}^pg_j(x_j).\footnote{Where $g_j$ is a real-valued function. For identifiability we also demand $\mathbb{E}[g_j(X_{:,j})] = 0$ for $j=1,\dots,p$.}
    \end{equation}  

    To estimate the non-parametric functions, we can use SS (cf. section~\ref{sec:Natural_SS}). For this let $\mathcal{S}_j$ be the function that takes some $z\in\R^n$ and returns the SS fitted to $(X_{:,j}, z)$ where the smoothing parameter is optimized by LOOCV\footnote{For efficiency a proxy of the LOOCV is used called generalized cross validation.}.
    Since we cannot fit all $g_j$ simultaneously, we will use a strategy named Backfitting. We basically cycle through the indices $1,\dots p$ and refit $\hat g_j$ each time. The following illustrates the procedure: 
    \begin{eqnarray*}
        1) \quad \hat g_1 &=& \mathcal S_1(y - \mu)    \\
        2) \quad \hat g_j &=& \mathcal S_j(y - \mu -\hat g_1(X_{[:,1]})-\dots -\hat g_{j-1}(X_{[:,{j-1}]})) \quad \text{for }j=2,\dots,p       \\
        3) \quad \hat g_1 &=& \mathcal S_1(y - \mu -\hat g_2(X_{[:,2]})-\dots -\hat g_p(X_{[:,p]}))       \\
        4) \quad \hat g_j &=& \mathcal S_j(y - \mu - \sum_{k\neq j}\hat g_k(X_{[:,k]})) \quad \text{for }j=2,\dots,p       \\
         & \vdots        
    \end{eqnarray*}
    We repeat step $3)$ and $4)$ until the change falls below some tolerance.

    \begin{my_pros_cons_table}{
        \item Captures non-linearity.
        \item Good interpretability.
    }{
        \item No automatic variable selection.
        \item Computationally expensive.
    }
    \end{my_pros_cons_table}
}

\subsection{Random Forest}{\label{sec:corr_model_RF}
    To define a Random Forests (RF) introduced by \cite{breimanRandomForests2001}  we will first define what a Tree is. A {(decision) Tree} is a graph $(V,E)$ without circles, a distinct root node, every node has at most two children and every leaf has a value assigned to it. At each node there is a boolean condition testing if one variable is greater than some value and a pointer to one child depending on the boolean value. To evaluate a tree we start at the root node, test the boolean expression and go to the node indicated by the resulting pointer. This we repeat until we end up at a leaf-node, where we return the value assigned to it. 
    
    To build such a Tree, we will recursively partition the covariate space using greedy splits\footnote{For computational reasons, we will only use splits along one covariate. So we `cut' our covariate space into rectangles.} decreasing the RMSE\footnote{To calculate the RMSE, we need a prediction. Let $P$ be the current partition, then the predicted value for some $x\in A \in P$ is the mean of the responses of all the points in $A$ (included in the training data).} each time. If the set we want to split contains less than a certain amount of training points, we stop.
    
    To build a {Random Forest} we will bootstrap-aggregate\footnote{That is we will sample (with replacement) several times n observations from our original data and fit a Tree to each such sample.} many such Trees\footnote{Building the Tree, this time we will not test every covariate at each node (for the RMSE minimization) but a node-specific subsample of the covariates. Thus, also the `second best split' can be selected.}. The prediction of the Random Forest for a new point $x$ is then the mean of the predictions from all the Trees. 
    \begin{my_pros_cons_table}{
        \item Captures non-linear relationships.
        \item Captures all interactions and performs automatic variable selection.
        \item Can deal with missing data.
    }{
        \item The resulting (prediction) function is not continuous, but locally constant.
        \item Computationally expensive.
        \item No interpretability.
    }
    \end{my_pros_cons_table}
}
\subsection{Multivariate Adaptive Regression Splines}{\label{sec:corr_model_MARS}
    A Multivariate Adaptive Regression Splines (MARS) model as introduced in \cite{friedmanMultivariateAdaptiveRegression1991} can be described by 
    \begin{equation}
        \label{eq:mars}
        g(x) = \sum_{m=0}^M \beta_m h_m(x),
    \end{equation}
    where the $h_m$ are simple functions (explained later) and the $\beta_m$ are estimated via Least Squares. 
    
    In the building procedure of a MARS model, we first select many of those simple functions and later drop some of them to avoid overfitting. For the construction of those simple functions, define $\mathcal{B}$ the set of pairs of `hockey stick functions' as:
    \begin{equation}
        \label{eq:mars_basis_fun}
        \mathcal{B}:=\left\{
            \left(b_1,b_2\right) 
            | \;
            \left(b_1(x),b_2(x)\right) = \left(\left(x_{j}-d\right)_+,\left(d-x_{j}\right)_+\right),
            %  x\in\R^p ,
            d =X_{1, j},  \ldots, X_{n, j},\;
            j=1, \ldots, p
        \right\}
    \end{equation}
    and the set $\mathcal{M}=\left\{1\right\}$ of all functions currently in the model. Now, consider $\mathcal{C}$ the set of candidate functions-pairs 
    \begin{equation}
        \label{eq:mars_candidate}
        \mathcal{C}:=\left\{
            \left(h(\cdot)b_1(\cdot),  h(\cdot)b_2(\cdot) \right)
            \;| \;\; h\in\mathcal{M}, \; 
            (b_1,b_2) \in \mathcal{B}
        \right\}
    \end{equation}
    and select the pair (which when added to $\mathcal{M}$ and the coefficients refitted) reduces the RMSE the most. Add the selected pair to $\mathcal{M}$ and repeat until the RMSE reduction becomes insignificant.

    Finally, to avoid overfitting, we prune the set $\mathcal{M}$ by optimizing a LOOCV score.\footnote{This means that we perform an iterative procedure to reduce the number of functions in $\mathcal{M}$. For every function $h$ in $\mathcal{M}$, we compute the model using $\mathcal{M}\\\{h\}$. We discard the function that -- when excluding from $\mathcal{M}$ -- leads to the best LOOCV score.}  

    To reduce computational complexity, we follow the recommendation from \cite{stephenEarthMultivariateAdaptive2021} and restrict $h$ in equation~\eqref{eq:mars_candidate} to be of degree one (so it is also in a pair of $\mathcal{B}$). Consequently, $\mathcal{C}$ contains functions with a degree of at most 2. 

    \begin{my_pros_cons_table}{
        \item Catches non-linear relationships.
        \item Interpretability via functions in $\mathcal{M}$ and their coefficients.
        \item Allows for interactions with variable selection.
    }{
        \item Computationally expensive (can be reduced by restricting the degree of interactions).
    }
    \end{my_pros_cons_table}
}
