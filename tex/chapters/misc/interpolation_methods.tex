% \todo[inline]{Paper zitieren wo eingeführt oder wo benutzt (falls einführung fast schon trivial)}

\section{Parametric Regression} 
	\label{sec:itpl_parametric}
	Parametric Curve estimation tries to fit a parametric function to a dataset, such as, a Gaussian function with parameters $\mu$ and $\sigma$. In the following, we introduce two parametric approaches.

	\subsection{Double Logistic}
		\label{sec:double_logistic}
		The Double Logistic (DL) smoothing as described in \cite{beckImprovedMonitoringVegetation2006} heavily relies on shape assumptions of the fitted curve (i.e., the NDVI {TS}). First, we assume that there  is a minimum NDVI level $y_{\min}$ in the winter (e.g., due to evergreen plants), which might be masked by snow. This can be estimated beforehand, taking several years into account. Second, we assume that the growth cycle can be divided into an increase and a decrease period, where both follow a logistic function. The fastest increase (or decrease) is observed at $t_0$ (or $t_1$) with a slope of $d_0$ (or $d_1$). The equation of the double-logistic fit is given by:
		\begin{equation*}
			y(t) = y_{\min} + \left(y_{\max}-y_{\min}\right)\left(\frac{1}{1+e^{-d_0(t-t_0)}}+\frac{1}{1+e^{-d_1(t-t_1)}}-1\right)
		\end{equation*}
		Where the five free parameters: $y_{\max}$, $d_0$, $d_1$, $t_0$, $t_1$ are estimated by least squares. Such fit can be seen in figure~\ref{fig:interpol/fourier_dl_comparison}.

		\subsubsection*{Robustification}
		Similar as for the SG (introduced in section~\ref{sec:Savitzky-Golay}), one can reestimate the parameters by giving less weight to the overestimated observations and more weight to the underestimated observations. For the details on the choice of the weights, we refer to \cite{beckImprovedMonitoringVegetation2006}. We will not apply this reestimation, but rather the robustification introduced later in section~\ref{sec:loess_robustify}.

		\begin{my_pros_cons_table}{
				\item Incorporates subject specific knowledge in the case of evergreen plants covered in snow.
				\item Optimized parameters have an intuitive meaning.
				\item Robust due to shape assumptions.
				\item Meaningful parameters.
			}{
				\item Strong shape assumptions limit flexibility.
				\item Parameter estimation can go wrong. Introducing bounds can help.
			}
		\end{my_pros_cons_table}


	\subsection{Fourier Series}
		\label{sec:fourier_approx}
		\cite{stockliEuropeanPlantPhenology2004} approximates the NDVI curve using a second order Fourier Series (FS):
		$$
			\operatorname{NDVI}(t)=\sum_{j=0}^{2} \Big(a_{j}  \cos \left(j  \Phi_{t}\right)+b_{j}  \sin \left(j  \Phi_{t}  \right)\Big)
		$$
		where $\Phi_t= T (t-1) / (2 \pi n)$. If we set the period $T$ to match one year, this would coincide with the notion that plants grow every year. 
		Analogous to section~\ref{sec:double_logistic} we fit the function above to the data via least squares.
		Example fits can be seen in figure~\ref{fig:interpol/fourier_dl_comparison}.


		% \cite{beckImprovedMonitoringVegetation2006} shows in their lag-plots a heavy autocorrelation of resiudals

		\begin{my_pros_cons_table}{
				\item Assumption of periodicity can be helpful if we are modelling multiyear grow cycles.
				\item Fourier Series are popular.
			}{
				\item Bad behavior in regions with little data (cf. figure~\ref{fig:interpol/fourier_dl_comparison}).
				\item Hard to interpret estimated parameters.
				\item Parameter estimation can go wrong. Introducing bounds can help.
			}
		\end{my_pros_cons_table}

		\begin{my_figure}[h]{width=1\textwidth}{interpol/fourier_dl_comparison}
			\caption[Fourier approximation and Double Logistic]{Here we observe the possibilities of a precise fit for the two parametric methods, but notice also some misbehavior.}
			\label{fig:interpol/fourier_dl_comparison}
		\end{my_figure}

	\subsection{Optimization Issues}\label{sec:itpl_param_optimizationissues}
		We shall mention some optimization issues we countered during implementation. Since we aim to minimize the residual sum of squares over 5 (or 6) parameters, we try to solve a non-convex optimization problem. Thus, the algorithm\footnote{We used the python function \texttt{scipy.optimize.curve\_fit}.} either struggles to find the global minimum or fails to converge. This was fixed by providing for each parameter reasonable initial values and generous bounds that match our experience (cf. appendix section~\ref{sec:app-param_bounds} for the bounds).

\section{Non-Parametric Regression}
	\label{sec:itpl_nonparametric}
	In non-parametric curve estimation, the curve does no longer have to be fully determined by parameters, but we allow it to flexibly approximate the data. Note that this does not exclude tuning-parameters.

	\subsection{Kernel Regression: Nadaraya-Watson}
		\label{sec:Kernel}
		As described in section~\ref{sec:itpl_setup}, we aim to estimate
		\begin{equation}
			\label{eq:nadaraya}
			\mathbb{E}[Y \mid T=t]
			= \int_{\R} y f_{Y \mid T}(y \mid t) d y
			=\frac{\int_{\R} y f_{T, Y}(t, y) d y}{f_{T}(t)},
		\end{equation}
		where $f_{Y \mid T}, f_{T, Y}, f_{T}$ denote the conditional, joint and marginal densities.
		This can be done with a kernel $K$:
		\begin{equation}
			\hat{f}_{T}(t)=\frac{\sum_{i=1}^{n} K\left(\frac{t-t_{i}}{h}\right)}{n h}, \quad \hat{f}_{T, Y}(t, y)=\frac{\sum_{i=1}^{n} K\left(\frac{t-t_{i}}{h}\right) K\left(\frac{y-Y_{i}}{h}\right)}{n h^{2}},
			\label{eq:kernel_with_bandwidt}
		\end{equation}
		where $h$, the bandwidth, symbolizes the window size of observations to consider. By using the above function in equation \eqref{eq:nadaraya}, we arrive at the Nadaraya-Watson kernel estimator (NW):
		$$\hat{m}(t)=\frac{\sum_{i=1}^{n} K\left(\left(t-t_{i}\right) / h\right) Y_{i}}{\sum_{i=1}^{n} K\left(\left(t-t_{i}\right) / h\right)}$$

		Common choices for the kernel are the normal function or a uniform function (also called `box' function). 
		\subsubsection*{Choose Bandwidth}
		Note, that we still need to choose the bandwidth of the function. This can be done with the help of LOOCV while optimizing the RMSE. For non-equidistant data we refer to \cite{brockmannLocallyAdaptiveBandwidth1993} where a local adaptive bandwidth selection is presented.

		\begin{my_pros_cons_table}{
				\item Flexible due to different possible kernels.
				\item Can be assigned degrees of freedom.
				\item Estimation of the noise variance $\hat \sigma_\varepsilon^2$.\footnotemark
			}{
				\item $\hat m $ is only continuous if $t \mapsto K(t)$ is continuous.
				\item Choice of bandwidth, especially if $t_i$ are not equidistant.
				\item Biased estimator. Underestimation (overestimation) for peaks (valleys).
			}
		\end{my_pros_cons_table}
		\footnotetext{{Cf. lecture notes of \cite{buhlmanComputationalStatistics2020} section 3.2.2.}}


	\subsection{Universal Kriging}
		\label{sec:Kriging}

		Universal Kriging (UK) as described in \cite{diggleGaussianModelsGeostatistical2007} was developed in geostatistics to deal with autocorrelation of the response variable at locations that are spatially close. By applying the notion that two spectral indices that are timewise close should also take similar values, we justify the application of UK. In the end, we would like to fit a smooth Gaussian process to the data.

		% definition (gaussian process)
		A Gaussian Process $\{S(t) : t\in \mathbb R\} $ is a stochastic process if $(S(t_1),\dots,S(t_k))$ has a multivariate Gaussian distribution for every collection of times ${t_1, \dots , t_k}$. $S$ can be fully characterized by the mean $\mu(t):=E[S(t)]$ and its covariance function $\gamma\left(t, t^{\prime}\right):=\operatorname{Cov}\left(S(t), S\left(t^{\prime}\right)\right)$. 
		% stationarity assumtpion
		Furthermore, we will assume the Gaussian process to be stationary. That is for $\mu(t)$ to be constant in $t$ and $\gamma(t,t')$ to depend only on $h=t-t'$. Thus, we will write in the following only $\gamma(h)$.\footnote{Note that the process is also {isotropic} (i.e., $\gamma(h)=\gamma(\|h\|$) since we are in a one-dimensional setting and the covariance is symmetric.}
		% variogram
		Now, we need to make some assumptions on the covariance function. For this we introduce the Variogram of a Gaussian process as
		$$V(h):=V\left(t, t+h\right):=\frac{1}{2} \operatorname{Var}\left(S(t)-S(t+h)\right)\\ %align XXX
			=\gamma(0) + \gamma(t)
		$$
		and define $\gamma$ via the above equation by choosing the Gaussian Variogram defined by
		$$V(h) = p \cdot\left(1-e^{-\frac{h^{2}}{\left(\frac{4}{7} r\right)^{2}}}\right)+n.$$
		Here $h$ denotes the distance, $n$ is the nugget, $r$ is the range and $p$ is the partial sill. The influence of the parameters is visualized in figure~\ref{fig:interpol/kriging_gauss_variogram}.\footnote{Strictly speaking, we use a scaled version of the Variogram. Thus, only the ratio of $p/n$ matters.}
		Finally, we consider a one-dimensional Gaussian process $G_\gamma$ with Variogram $\gamma$ and tune the Variogram parameters using maximum likelihood. {As illustrated in figure~\ref{fig:kriging_parameters} maximum likelihood estimation can lead to overfitting. Thus, we will in practice sample several such optimized parameters and use their median in the end.} Let $z$ be a vector with the new values to extrapolate, then we can determine the values $m(z) = \mathbb{E}\left[G_\gamma(z) | (t,y)\right]$ using Bayes rule\footnote{Bayes rule claims that for two random variables $A$, and $B$ we have that $P(A|B) = P(B|A) / P(B)$.}. For an example fit, we refer to figure~\ref{fig:kriging_parameters}. 
		    \begin{my_figure}[h]{width=0.7\textwidth}{interpol/kriging_gauss_variogram}
				\caption[Gaussian Variogram]{Gaussian Variogram with nugget=1, partial sill=3, range=55}
				\label{fig:interpol/kriging_gauss_variogram}
			\end{my_figure}
			\begin{my_figure}{width=1\textwidth}{interpol/kriging_parameter}
				\caption[Effect of variogram parameters and failure of maximum likelihood.]{On the left, we see how the interpolation changes if we increase the nugget and the range parameter. On the right, we compare two UK interpolations, where one takes parameters by numerically maximizing the (which results in a very small nugget) and the other takes the median of many such numerical optimizations.}
				\label{fig:kriging_parameters}
			\end{my_figure}
		\subsubsection*{Violated Assumption}
			Since we observe a clear pattern of a growth period in spring and harvest in the end of summer, we conclude that our stationarity assumption with the constant mean is structurally violated. This is also why we observe a tendency to the mean, as indicated in figure~\ref{fig:kriging_parameters}, regardless of range parameter.

		\begin{my_pros_cons_table}{
				\item It is a well-studied method.
				\item Informative variogram parameters.
				\item Flexible covariance structure.
			}{
				\item Violated stationarity assumption (constant mean and constant variance). 
				\item Tendency to the mean (especially within data gaps).
				\item Pure maximum likelihood can result in overfitting.
			}
		\end{my_pros_cons_table}


	\subsection{Savitzky-Golay Filter}
		\label{sec:Savitzky-Golay}
		The Savitzky-Golay Filter (SG), introduced in \cite{savitzkySmoothingDifferentiationData1964} is a technique in signal processing and can be used as a low-pass filter  \citep{schaferWhatSavitzkyGolayFilter2011}. Hence, it can also be used for smoothing by filtering high frequency noise while keeping the low frequency signal.

		First, we choose a window size $m$. Then, for each point, $j \in \{m, m+1, \dots, n-m\}$ we fit a polynomial of degree $k$ by:
		$$\hat y_j=\min_{p\in P_k}\sum_{i=-m}^{m}(p (t_{j+i})-y_{i+j})^{2},$$
		where $P_k$ denotes the Polynomials of degree $k$ over $\R$.
		For equidistant points this can efficiently be calculated by
		$$
			\hat y_{j}=\sum_{i=-m}^{m} c_{i} y_{j+i},
		$$
		where the $c_i$ are only dependent on the $m$ and $k$ and are tabulated in the original paper.

		\cite{chenSimpleMethodReconstructing2004a} developed a `robust' {{IM}} variant of the SG. 
		The method is based on the assumption that due to atmospheric effects the observed NDVI tends to be underestimated and that it cannot increase too quickly. The latter is argued by the biological impossibility of such fast vegetation changes. Their proposed algorithm is:
			\begin{enumerate}
				\item Remove non-SCL45 points.
				\item Remove points that would indicate an increase greater than 0.4 within 20 days.
				\item Linearly interpolate to obtain an equidistant {TS} $X^0$.
				\item Apply the SG to obtain a new {TS} $X^1$.
				\item Set $X^0\leftarrow X^1$ and update $X^1$ by applying again the SG. Repeat this until $w^T |X^1-X^0|$ stops decreasing, where w is a weight vector that is only used for evaluation and not for fitting and is calcualted by $w_i = \min\left(1, 1 - \frac{X^1_i-X^0_i}{\max_i\|X^1_i-X^0_i\|}\right)$. This reduces the penalty introduced by outliers\footnote{Here we call a point $i$ an outlier if $X^0_i<X^1_i$.} and by repeating this step we approach the `upper NDVI envelope'.
			\end{enumerate}

		\subsubsection*{Extension: Spatial-Temporal SG}
			One notable adaptation of the SG is the presented by \cite{caoSimpleMethodImprove2018b}. The key difference is the additional assumption of the cloud cover being discontinuous and that we can improve by looking at adjacent\footnote{Here, we say that a pixel is adjacent if it is the same pixel but from a different year (keeping the same day of the year) or (if not enough of such temporal-adjacent pixel are found) it is spatially adjacent.} pixels. Because we are working with rather high-resolution satellite data, and we need the variance in the predictors, we will waive this extension.

		\begin{my_pros_cons_table}{
				\item Popular technique in signal processing.
				\item Efficient calculation for equidistant points.
				\item The upper envelope matches intuition for the NDVI. Therefore, it is robust against outliers with small values.
			}{
				\item No natural way of how to estimate points that are not in the data.
				\item Not generalizable to other spectral indices.
				\item Linear interpolation to account for missing data might not be appropriate.
				\item No continuous interpolation between two measurements.
			}
		\end{my_pros_cons_table}


	\subsection{Locally Weighted Regression}
		\label{sec:loess}
		% Introduced by : \cite{clevelandRobustLocallyWeighted1979}
		% implemented here \cite{cappellariATLAS3DProjectXX2013}

		The Locally Weighted Regression (LOESS) introduced by \cite{clevelandRobustLocallyWeighted1979} can be understood as a generalization of the SG (cf. section~\ref{sec:Savitzky-Golay}).

		Given a proportion $\alpha \in (0,1]$, we estimate each $y_i$ separately by fitting a polynomial of order $d$ by weighted least squares. The weights are usually\footnote{Other functions are also possible choices as long as they satisfy the conditions stated in \cite{clevelandRobustLocallyWeighted1979}.} defined by
		$$w_i(t_j)=\begin{cases}
				\left(1-\left(\frac{|t_j-t_i|}{h_i}\right)^{3}\right)^{3}, & \text{for } |t_j-t_i|<h_i           \\
				0,                                                   & \text{for } |t_j-t_i| \geqslant h_i
			\end{cases} ,$$
		where $h_i$ is the minimal distance such that $\lceil \alpha n\rceil$ observations are in the ball $B_{h_i}(t_i)$.\footnote{\label{footnote:LOESS}If too many weights are set to zero, we might end up considering not enough observations and thus get a singular design-matrix (for the least squares estimation). To extend good behavior, we substitute $h_i$ with $1.01 h_i$, so that the observations on the boundary of $B_{h_i}(t_i)$ do not get completely ignored. But we also have to assure that $\alpha$ is big enough.} So for each $y_i$, we only consider a proportion $\alpha$ of the observations.

		\subsubsection{Differences between the Robust LOESS and the SG}
		The LOESS smoother takes a fraction of points instead of a fixed number, and therefore automatically adapts to the size of the data we wish to interpolate. However, we run into the danger of considering too little observations, since the estimation breaks down if $\lceil \alpha n\rceil < d+1$.\footnoteref{footnote:LOESS}
		Furthermore, LOESS gives less weight to points further away. This yields a `smoother' estimate, since when we slide the window (e.g., for estimating the next value) an influential point at the border does not suddenly get zero weight from being weighted equally before.
		Finally, the LOESS can also be used for non-equidistant data and allows for arbitrary interpolation.

		\begin{my_pros_cons_table}{
				\item Flexible generalization of SG.
				\item Arbitrary interpolation possible.
				\item Intuitive parameters.
			}{
				\item The nature of local regression might lead to surprising estimates (no smoothness guarantees for the second derivative).
			}
		\end{my_pros_cons_table}


	\subsection{B-Splines}
		\label{sec:B}
		B-Splines (BS) as discussed in \cite{lycheSplineMethods2005} are piecewise cubic polynomials defined by 
		$$
			S(t)=\sum_{j=0}^{n-1} c_{j} B_{j, k ; t}(t),
		$$
		where $B$ are basis functions and recursively defined by:
		
		\begin{equation}
				B_{i, 0}(z)=1, \text { if } t_{i} \leq z<t_{i+1}, \text { otherwise } 0 \\
		\end{equation}
		\begin{equation}
			B_{i, k}(z)=\frac{z-t_{i}}{t_{i+k}-t_{i}} B_{i, k-1}(z)+\frac{t_{i+k+1}-z}{t_{i+k+1}-t_{i+1}} B_{i+1, k-1}(z).
		\end{equation}
		
		Assuming that all $t_i$ are distinct, this yields an interpolation that fits the data perfectly. To reduce the amount of overfitting and increase the smoothness, we relax the constraint that we must interpolate perfectly. Thus, we use the minimum number of basis functions\footnote{So we do not require one basis function for each neighboring pair of knots. SciPy uses FITPACK and DFITPACK, the documentation suggests that smoothness is achieved by reducing the number of knots used.} such that keeps the sum of squares below some threshold $\lambda$:
		$$\sum_{i=1}^n(w_i (y_i - \hat y_i))^2 \leq \lambda$$

		\begin{my_pros_cons_table}{
				\item Can be assigned degrees of freedom.
				\item Extendable to a `smooth' version.
				\item Also performs well if points are not equidistant.
			}{
				\item Smoothing process does not translate well to an interpretation (unlike SS).
				\item Choice of smoothing parameter $\lambda$.
			}
		\end{my_pros_cons_table}


	\subsection{Smoothing Splines}
		\label{sec:Natural_SS}
		To define the Smoothing Splines (SS) as in \cite{cravenSmoothingNoisyData1978}, let $\mathcal F$ be the Sobolev space (i.e., the space of functions of which the second derivative is integrable). Then the SS are the unique minimizer of
		\begin{equation}
			\label{eq:ss}
			\hat m :=\argmin_{f \in \mathcal F} \sum_{i=1}^{n}w_i\left(y_{i}-{f}\left(t_{i}\right)\right)^{2}+\lambda \int {f}^{\prime \prime}(t)^{2} dt,\quad \text{ for } \lambda >0.
		\end{equation}
		 % natural\footnote{It is called natural since it is affine outside the data range ($\forall t\notin [t_1, t_n]:\hat m''(t) = 0$)} 
		 The objective function ensures that we decrease the curvature while keeping the RMSE low.
		SS can be represented as cubic splines i.e., piecewise cubic polynomial functions.

		\subsubsection{Whittaker---Discrete Version with Higher Order Derivatives}
			The Whittaker smoother introduced in \cite{eilersPerfectSmoother2003} is closely reminiscent of the SS and is also used for the NDVI TS \citep{atzbergerTimeSeriesMonitoring2011}. Similar to SS, we minimize the following expression over $z\in \R^n$:
			$$
			({y}-{z})^{{T}} {W}({y}-{z})+\lambda {z}^{{T}} {D}^{{T}} {D} \mathbf{z},
			$$
			where $W$ is a diagonal weight-matrix, $\lambda$ our smoothing parameter and $D$ a matrix that serves the purpose of approximating a differentiation of $k^\text{th}$ order. In essence, this minimization function is the same as equation~\refeq{eq:ss}. The only differences are, that we substitute the integral by a sum and that we are more flexible with the order of the derivatives we are using. The main drawback is that we do not get a smooth function that interpolates, and that the sum behaves worse than the integral for non-equidistant data points. Thus, we will not consider the Whittaker further in favor of the more general SS. 

		\begin{my_pros_cons_table}{
				\item Can be assigned degrees of freedom.
				\item Efficient estimation (closed form solution).
				\item Intuitive penalty reduces curvature.
				\item Performs well if points are not equidistant.
				\item Fixes the Runge's phenomenon\footnotemark.
				\item Bounded within the data range if $\lambda$ is chosen a priori.
			}{
				\item The tuning parameter $\lambda$ must be chosen. This can be done via cross validation and optimizing a score function (e.g., the RMSE). 
			}
		\end{my_pros_cons_table}
		\footnotetext{{I.e., fluctuation of high degree polynomial interpolation.}}


	% \subsection{XXX Whittaker Smoother}

	% \label{sec:whittaker}
	% XXX
	% from [HERE](https://eigenvector.com/wp-content/uploads/2020/01/WhittakerSmoother.pdf):  
	%     The Whittaker Smoother: Eiler's paper[1] introduces the following objective function
	    % $$
	    % O(\mathbf{z})=(\mathbf{y}-\mathbf{z})^{\mathrm{T}} \mathbf{W}_{0}(\mathbf{y}-\mathbf{z})+\lambda_{\mathrm{s}} \mathbf{z}^{\mathrm{T}} \mathbf{D}_{\mathrm{s}}^{\mathrm{T}} \mathbf{D}_{\mathrm{s}} \mathbf{z}
	    % $$
	%     where $\mathbf{y}$ is a $N \cdot 1$ vector of measured data, $\mathbf{z}$ is smooth curve to be fit to the data, $\mathbf{W}_{0}$ is a diagonal matrix of weights (typically $0 \leq w_{0, n} \leq 1$ for $n=1, \ldots, N, \mathbf{D}_{\mathrm{s}}$ is a second derivative operator (e.g., $\mathbf{D}_{\mathrm{s}} \mathbf{z}$ is the second derivative of $\mathbf{z}$ ) and $\lambda_{\mathrm{s}}$ is a scalar penalty on the smoothing term. When data are missing, the corresponding weight, $w_{0, n}$, can be set to zero. Once that $\mathbf{W}_{0}$ and $\lambda_{\mathrm{s}}$ are given (set by default or provided by the user) the corresponding estimate of $\mathbf{z}$ is given by
	%     $$
	%     \hat{\mathbf{z}}=\left(\mathbf{W}_{0}+\lambda_{\mathrm{s}} \mathbf{D}_{\mathrm{s}}^{\mathrm{T}} \mathbf{D}_{\mathrm{s}}\right)^{-1} \mathbf{W}_{0} \mathbf{y}
	%     $$
	%     For example, an optical emission (OES) spectrum is plotted Figure 1 along with two smoothed versions shown for $\mathbf{W}_{0}=\mathbf{I}$ and $\lambda_{\mathrm{s}}=0.1$ (low smoothing) and $\lambda_{\mathrm{s}}=10$ (stronger smoothing).   
	% **Original paper states use of the first derivative**  
	% --> second derivative is very similar to SS
	% \begin{my_pros_cons_table}{
	%     \item 1
	%     \item 2
	%   }{
	%     \item 1
	%     \item 2
	%   }
	% \end{my_pros_cons_table}
