\todo[inline]{Paper zitieren wo eingeführt oder wo benutzt (falls einführung fast schon trivial)}

\section{Parametric Regression} 
	\label{sec:itpl_parametric}
	Parametric Curve estimation tries to fit a parametric function, such as, for example, a Gaussian function with parameters $\mu$ and $\sigma$, to a dataset. In the following, we introduce two parametric approaches.

	\subsection{Double Logistic (DL)}
		\label{sec:double_logistic}
		The Double Logistic smoothing as described in \cite{beckImprovedMonitoringVegetation2006} heavily relies on shape assumptions of the fitted curve (i.e., the NDVI {TS}). First, we assume that there  is a minimum NDVI level $y_{\min}$ in the winter (e.g., due to evergreen plants), which might be masked by snow. This can be estimated beforehand, taking several years into account. Second, we assume that the growth cycle can be divided into an increase and a decrease period, where the {TS} follows a logistic function. The maximum increase (or decrease) is observed at $t_0$ (or $t_1$) with a slope of $d_0$ (or $d_1$). The equation of the double-logistic fit is given by:
		\begin{equation*}
			y(t) = y_{\min} + \left(y_{\max}-y_{\min}\right)\left(\frac{1}{1+e^{-d_0(t-t_0)}}+\frac{1}{1+e^{-d_1(t-t_1)}}-1\right)
		\end{equation*}
		Where the five free parameters: $y_{\max}$, $d_0$, $d_1$, $t_0$, $t_1$ are initially estimated by least squares. Such fit can be seen in figure~\ref{fig:interpol/fourier_dl_comparison}.

		\subsubsection*{Robustification}
		Similar as for the SG (cf. section~\ref{sec:Savitzky-Golay}) one can reestimate (only once) the parameters by giving less weight to the overestimated observations and more weight to the underestimated observations. For the details on the choice of the weights we refer to \cite{beckImprovedMonitoringVegetation2006}. We will not apply this reestimation but rather the robustification introduced later in section \ref{sec:loess_robustify}.

		\begin{my_pros_cons_table}{
				\item Incorporates subject specific knowledge in the case of evergreen plants covered in snow.
				\item Optimized parameters have an intuitive meaning.
			}{
				\item Strong shape assumptions on the NDVI curve.
				\item Parameter optimization might go wrong. This can be mitigated to some extent to provide bounds for the parameters
				\item Strange behavior in regions with little observations. (cf. figure~\ref{fig:interpol/fourier_dl_comparison})
			}
		\end{my_pros_cons_table}


	\subsection{Fourier Series (FS)}
		\label{sec:fourier_approx}
		\cite{stockliEuropeanPlantPhenology2004} approximates the NDVI curve using a second order FS:
		$$
			\operatorname{NDVI}(t)=\sum_{j=0}^{2} a_{j} \times \cos \left(j \times \Phi_{t}\right)+b_{j} \times \sin \left(j \times \Phi_{t}  \right)
		$$
		where $\Phi=2 \pi \times(t-1) / n$. Thus, we periodical behavior. If we would set the period to match one year this would coinced with the notion that plans grow every year. 
		Analogous to section~\ref{sec:double_logistic} we fit it to the data by least squares.
		Example fits can be seen in figure \ref{fig:interpol/fourier_dl_comparison}


		% \cite{beckImprovedMonitoringVegetation2006} shows in their lag-plots a heavy autocorrelation of resiudals

		\begin{my_pros_cons_table}{
				\item Assumption of periodicity can be helpful if we are modelling multiyear grow cycles
				\item Flexible curve shape
			}{
				\item Bad behavior in regions with little data (cf. figure~\ref{fig:interpol/fourier_dl_comparison})
				\item Hard to interpret estimated parameters
				\item Parameter estimation can go wrong. Introducing bounds can help.
			}
		\end{my_pros_cons_table}

		\begin{my_figure}[h]{width=1\textwidth}{interpol/fourier_dl_comparison}
			\caption{Here we observe the possibilities of a precise fit for the two parametric methods but notice also some misbehavior}
			\label{fig:interpol/fourier_dl_comparison}
		\end{my_figure}

	\subsection{Optimization Issues}\label{sec:itpl_param_optimizationissues}
		We shall mention some optimization issues we countered during implementation. Since we aim to minimize the residual sum of squares over 5 (or 6) parameters, we try to solve a non-convex optimization problem. Thus, the algorithm\footnote{We used the python function \texttt{scipy.optimize.curve\_fit}.} either struggles to find the global minimum or fails to converge. This was fixed by providing for each parameter reasonable initial values and generous bounds (that match our experience).

\section{Non-Parametric Regression}
	\label{sec:itpl_nonparametric}
	In non-parametric curve estimation, the curve does no longer have to be fully determined by parameters, but we allow it to flexibly approximate the data. Note that we do not exclude the use of tuning-parameters.

	\subsection{Kernel Regression: Nadaraya-Watson (NW)}
		\label{sec:Kernel}
		As described in section \ref{sec:itpl_setup}, we aim to estimate
		\begin{equation}
			\label{eq:nadaraya}
			\mathbb{E}[Y \mid T=t]
			= \int_{\R} y f_{Y \mid T}(y \mid t) d y
			=\frac{\int_{\R} y f_{T, Y}(t, y) d y}{f_{T}(t)},
		\end{equation}
		where $f_{Y \mid T}, f_{T, Y}, f_{T}$ denote the conditional, joint and marginal densities.
		This can be done with a kernel $K$:
		\begin{equation}
			\hat{f}_{T}(t)=\frac{\sum_{i=1}^{n} K\left(\frac{t-t_{i}}{h}\right)}{n h}, \quad \hat{f}_{T, Y}(t, y)=\frac{\sum_{i=1}^{n} K\left(\frac{t-t_{i}}{h}\right) K\left(\frac{y-Y_{i}}{h}\right)}{n h^{2}},
			\label{eq:kernel_with_bandwidt}
		\end{equation}
		where $h$, the bandwidth, symbolizes the windowsize of to consider. By using the above function in equation \eqref{eq:nadaraya} we arrive at the NW kernel estimator:
		$$\hat{m}(t)=\frac{\sum_{i=1}^{n} K\left(\left(t-t_{i}\right) / h\right) Y_{i}}{\sum_{i=1}^{n} K\left(\left(t-t_{i}\right) / h\right)}$$

		Common choices for the kernel are the normal function or a uniform function (also called `bot' function). 
		\subsubsection*{Choose Bandwidth}
		Note that we still need to choose the bandwidth of the function. This can be done with the help of LOOCV while optimizing the RMSE. For non-equidistant data we refere to \cite{brockmannLocallyAdaptiveBandwidth1993} where a local adaptive bandwidth selection is presented.

		\begin{my_pros_cons_table}{
				\item fletible due to different possible kernels
				\item can be assigned degrees of freedom (trace of the hat-matrit)
				\item estimation of the noise variance $\hat \sigma_\varepsilon^2$ (REF cf. CompStat 3.2.2)
			}{
				\item if the $t \mapsto K(t)$ is not continuous, $\hat m $ isn't either
				\item choice of bandwidth, especially if $t_i$ are not equidistant.
			}
		\end{my_pros_cons_table}


	\subsection{Universal Kriging (UK)}
		\label{sec:Kriging}

		UK as described in \cite{diggleGaussianModelsGeostatistical2007} was developed in geostatistics to deal with autocorrelation of the response variable at locations that are spatially close. By applying the notion that two spectral indices that are timewise close should also take similar values, we justify the application of UK. In the end, we would like to fit a smooth Gaussian process to the data.

		% definition (gaussian process)
		A Gaussian Process $\{S(t) : t\in \mathbb R\} $ is a stochastic process if $(S(t_1),\dots,S(t_k))$ has a multivariate Gaussian distribution for every collection of times ${t_1, \dots , t_k}$. $S$ can be fully characterized by the mean $\mu(t):=E[S(t)]$ and its covariance function $\gamma\left(t, t^{\prime}\right):=\operatorname{Cov}\left(S(t), S\left(t^{\prime}\right)\right)$. 
		% stationarity assumtpion
		Furthermore, we will assume the Gaussian process to be stationary. That is for $\mu(t)$ to be constant in $t$ and $\gamma(t,t')$ to depend only on $h=t-t'$. Thus, we will write in the following only $\gamma(h)$.\footnote{Note that the process is also \textit{isotropic} (i.e., $\gamma(h)=\gamma(\|h\|$) since we are in a one-dimensional setting and the covariance is symmetric.}

		% variogram
		Now, we need to make some assumption on the covariance function. For this we introduce the variogram of a Gaussian process as
		$$V(h):=V\left(t, t+h\right):=\frac{1}{2} \operatorname{Var}\left(S(t)-S(t+h)\right)\\ %align XXX
			=\gamma(0) + \gamma(t)
		$$
		and define $\gamma$ via the above equation by choosing the Gaussian Variogram defined by
		$$V(h) = p \cdot\left(1-e^{-\frac{h^{2}}{\left(\frac{4}{7} r\right)^{2}}}\right)+n.$$
		Here $h$ denotes the distance, $n$ is the nugget, $r$ is the range and $p$ is the partial sill. The influence of the parameters is visualized in figure~\ref{fig:interpol/kriging_gauss_variogram}.\footnote{Strictly speaking we use a scaled version of the variogram. Thus, only the ratio of $p/n$ matters.}

			\begin{my_figure}[h]{width=0.7\textwidth}{interpol/kriging_gauss_variogram}
				\caption{Gaussian Variogram with nugget=1, partial sill=3, range=55}
				\label{fig:interpol/kriging_gauss_variogram}
			\end{my_figure}

			\begin{my_figure}{width=1\textwidth}{interpol/kriging_parameter}
				\caption{On the left, we see how the interpolation change if we increase the nugget and the range parameter. On the right, we compare two UK interpolations, where one takes parameters by numerically maximizing the (which results in a very small nugget) and the other takes the median of many such numerical optimizations.}
				\label{fig:kriging_parameters}
			\end{my_figure}

		Finally, we consider a one-dimensional Gaussian process $G_\gamma$ with variogram $\gamma$ and tune the variogram parameters using matimum likelihood\footnote{As illustrated in figure~\ref{fig:kriging_parameters} matimum likelihood estimation can lead to overfitting. Thus, we will in practice sample several such optimized parameters and use their median in the end.}. Let $z$ be a vector with the new values to ettrapolate, then we can determine the values $m(z) = \mathbb{E}\left[G_\gamma(z) | (t,y)\right]$ using Bayes rule\footnote{Bayes rule generally claims that for two random variables $A$ and $B$ we have that $P(A|B) = P(B|A) / P(B)$}. For an etample fit, we refer to figure~\ref{fig:kriging_parameters}. 

		\subsubsection*{Violated Assumption}
			Since we observe a clear pattern of a growth period in spring and harvest in the end of summer, we have to admit that our stationarity assumtpion with the constant mean is structurally violated. This is also the reason why we observe (for every variogram parameter) a tendency to the mean, as indicated in figure~\ref{fig:kriging_parameters}.

		\begin{my_pros_cons_table}{
				\item It is a well-studied method.
				\item Variogram parameters have an intuitive meaning.
				\item Flexible covariance structure.
			}{
				\item Regression to the mean.
				\item Violated assumption of constant mean and constant variance. Thus, the NDVI is not a stationary process.
				\item Pure maximum likelihood can result in overfitting.
			}
		\end{my_pros_cons_table}


	\subsection{Savitzky-Golay Filter (SG)}
		\label{sec:Savitzky-Golay}
		The SG, introduced in \cite{savitzkySmoothingDifferentiationData1964} is a technique in signal processing and can be used to filter out high frequencies (low-pass filter) \citep{schaferWhatSavitzkyGolayFilter2011}. Furthermore, it can also be used for smoothing by filtering high frequency noise while keeping the low frequency signal.

		First, we choose a window size $m$. Then, for each point, $j \in \{m, m+1, \dots, n-m\}$ we fit a polynomial of degree $k$ by:
		$$\hat y_j=\min_{p\in P_k}\sum_{i=-m}^{m}(p (t_{j+i})-y_{i+j})^{2},$$
		where $P_k$ denotes the Polynomials of degree $k$ over $\R$.
		For equidistant points this can efficiently be calculated by
		$$
			\hat y_{j}=\sum_{i=-m}^{m} c_{i} y_{j+i},
		$$
		where the $c_i$ are only dependent on the $m$ and $k$ and are tabulated in the original paper.

		\cite{chenSimpleMethodReconstructing2004a} developed a `robust' {{IM}} for the NDVI based on the SG. 
		The method is based on the assumption that due to atmospheric effects the observed NDVI tends to be underestimated and that it cannot increase too quickly. The latter is argued by the biological impossibility of such fast vegetation changes. Their proposed algorithm is:\todo{figure / tabelle / pseudocode anstatt aufzählung}
			\begin{enumerate}
				\item Remove non-SCL45 points.
				\item Remove points that would indicate an increase greater than 0.4 within 20 days.
				\item Linearly interpolate to obtain an equidistant {TS} $X^0$.
				\item Apply the SG to obtain a new {TS} $X^1$.
				\item Update $X^1$ by applying again a SG. Repeat this until $w^T |X^1-X^0|$ stops decreasing, where w is a weight vector with $w_i = \min\left(1, 1 - \frac{X^1_i-X^0_i}{\max_i\|X^1_i-X^0_i\|}\right)$. This reduces the penalty introduced by outliers\footnote{Here we call a point $i$ an outlier if $X^0_i<X^1_i$.} and by repeating this step we approach the ``upper NDVI envelope''.
			\end{enumerate}

		\subsubsection*{Extension: Spatial-Temporal SG}
			One notable adaptation of the SG is the presented by \cite{caoSimpleMethodImprove2018b}. The key difference is the additional assumption of the cloud cover being discontinuous and that we can improve by looking at adjacent pixels\footnote{Here, we say that a pixel is adjacent if it is the same pixel but from a different year (keeping the same day of the year) or (if not enough of such temporal-adjacent pixel are found) it is spatially adjacent}. Because we are working with rather high resolution satellite data, and we need the variance in the predictors, we will waive this extension.

		\begin{my_pros_cons_table}{
				\item Popular technique in signal processing.
				\item Efficient calculation for equidistant points.
				\item Upper envelope matches intuition for the NDVI. Therefore, it is robust against outliers with small values.
			}{
				\item No natural way of how to estimate points that are not in the data.
				\item Not generalizable to other spectral indices.
				\item Linear interpolation to account for missing data might be not appropriate.
				\item No smooth interpolation between two measurements.
			}
		\end{my_pros_cons_table}


	\subsection{Locally Weighted Regression (LOESS)}
		\label{sec:loess}
		% Introduced by : \cite{clevelandRobustLocallyWeighted1979}
		% implemented here \cite{cappellariATLAS3DProjectXX2013}

		The LOESS introduced by \cite{clevelandRobustLocallyWeighted1979} can be understood as a generalization of the SG (cf. sec.~\ref{sec:Savitzky-Golay}).

		Given a proportion $\alpha \in (0,1]$, we estimate each $y_i$ separately by fitting a polynomial of order $d$ by weighted least squares. The weights are (usually) defined by
		$$w_i(t_j)=\begin{cases}
				\left(1-\left(\frac{|t_j-t_i|}{h_i}\right)^{3}\right)^{3}, & \text{for } |t_j-t_i|<h_i           \\
				0,                                                   & \text{for } |t_j-t_i| \geqslant h_i
			\end{cases} ,$$
		where $h_i$ is the minimal distance such that $\lceil \alpha n\rceil$ observations are in the ball $B_{h_i}(t_i)$.\footnote{\label{footnote:LOESS}If too many weights are set to zero, we might end up considering not enough observations and thus get a singular design-matrit (for the least squares estimation). Therefore, we substitute $h_i$ with $1.01 h_i$, so that the observation on the boundary of $B_{h_i}(t_i)$ does not get completely ignored. But we also have to assure that $\alpha$ is big enough.} So for each $y_i$ we only consider a proportion $\alpha$ of the observations.

		\subsubsection{Differences between the Robust LOESS and the SG}
		The LOESS smoother takes a fraction of points instead of a fixed number and therefore automatically adapts to the size of the data we wish to interpolate. However, we run into the danger of considering too little observations, since the estimation breaks down if $\lceil \alpha n\rceil < d+1$.\footnoteref{footnote:LOESS}
		Furthermore, LOESS gives less weight to points further away. This yields a "smoother" estimate, since when we slide the window (e.g., for estimating the next value) an influential point at the border does not suddenly get zero weight from being weighted equally before.
		Finally, the LOESS also can be used for non-equidistant data and allows for arbitrary interpolation.

		\begin{my_pros_cons_table}{
				\item Flexible generalization of SG
				\item arbitrary interpolation possible
				\item Intuitive parameters
			}{
				\item The nature of local regression might lead to surprising estimates (no smoothness guarantees for the second derivative)
			}
		\end{my_pros_cons_table}


	\subsection{B-Splines (BS)}
		\label{sec:B}
		BS as discussed in \cite{lycheSplineMethods2005} are piecewise cubic polynomials defined by 
		$$
			S(t)=\sum_{j=0}^{n-1} c_{j} B_{j, k ; t}(t),
		$$
		where $B$ are basis functions and recursively defined by:
		
		\begin{equation}
				B_{i, 0}(z)=1, \text { if } t_{i} \leq z<t_{i+1}, \text { otherwise } 0 \\
		\end{equation}
		\begin{equation}
			B_{i, k}(z)=\frac{z-t_{i}}{t_{i+k}-t_{i}} B_{i, k-1}(z)+\frac{t_{i+k+1}-z}{t_{i+k+1}-t_{i+1}} B_{i+1, k-1}(z).
		\end{equation}
		
		Assuming that all $t_i$ are distinct, this yields an interpolation that fits the data perfectly. To reduce the amount of overfitting and increase the smoothness, we relax the constraint that we have to perfectly interpolate. Thus, we use the minimum number of basis functions\footnote{So we do not require one basis function for each neighboring pair of knots. SciPy uses FITPACK and DFITPACK, the documentation suggests that smoothness is achieved by reducing the number of knots used} such that:
		$$\sum_{i=1}^n(w_i (y_i - \hat y_i))^2 \leq s$$

		\begin{my_pros_cons_table}{
				\item can be assigned degrees of freedom
				\item extendable to "smooth" version
				\item performs also well if points are not equidistant
			}{
				\item smoothing process does not translate well to a interpretation (unlike SS)
				\item choice of smoothing parameter $s$
			}
		\end{my_pros_cons_table}


	\subsection{Smoothing Splines (SS)}
		\label{sec:Natural_SS}
		Let $\mathcal F$ be the Sobolev space (the space of functions of which the second derivative is integrable). Then the unique\footnote{Strictly speaking it is only unique for $\lambda > 0$} minimizer
		\begin{equation}
			\label{eq:ss}
			\hat m :=\argmin_{f \in \mathcal F} \sum_{i=1}^{n}w_i\left(y_{i}-{f}\left(t_{i}\right)\right)^{2}+\lambda \int {f}^{\prime \prime}(t)^{2} dt
		\end{equation}
			
		is a % natural\footnote{It is called natural since it is affine outside the data range ($\forall t\notin [t_1, t_n]:\hat m''(t) = 0$)} 
		cubic spline (i.e., a piecewise cubic polynomial function).
		The objective function ensures that we decrease the curvature while keeping the RMSE low.

		\subsubsection{XXX Whittaker}

		\begin{my_pros_cons_table}{
				\item Can be assigned degrees of freedom (trace of the hat-matrix).
				\item Efficient estimation (closed form solution).
				\item Intuitive penalty (we don't want the function to be too ``wobbly'' --- change slopes).
				\item Also performs well if points are not equidistant.
				\item Fixes the Runge's phenomenon (fluctuation of high degree polynomial interpolation).
			}{
				\item The tuning parameter $\lambda$ must be chosen. This can be done via cross validation and optimizing a score function (e.g., the RMSE). 
			}
		\end{my_pros_cons_table}


	% \subsection{XXX Whittaker Smoother}

	% \label{sec:whittaker}
	% XXX
	% from [HERE](https://eigenvector.com/wp-content/uploads/2020/01/WhittakerSmoother.pdf):  
	%     The Whittaker Smoother: Eiler's paper[1] introduces the following objective function
	%     $$
	%     O(\mathbf{z})=(\mathbf{y}-\mathbf{z})^{\mathrm{T}} \mathbf{W}_{0}(\mathbf{y}-\mathbf{z})+\lambda_{\mathrm{s}} \mathbf{z}^{\mathrm{T}} \mathbf{D}_{\mathrm{s}}^{\mathrm{T}} \mathbf{D}_{\mathrm{s}} \mathbf{z}
	%     $$
	%     where $\mathbf{y}$ is a $N \times 1$ vector of measured data, $\mathbf{z}$ is smooth curve to be fit to the data, $\mathbf{W}_{0}$ is a diagonal matrix of weights (typically $0 \leq w_{0, n} \leq 1$ for $n=1, \ldots, N, \mathbf{D}_{\mathrm{s}}$ is a second derivative operator (e.g., $\mathbf{D}_{\mathrm{s}} \mathbf{z}$ is the second derivative of $\mathbf{z}$ ) and $\lambda_{\mathrm{s}}$ is a scalar penalty on the smoothing term. When data are missing, the corresponding weight, $w_{0, n}$, can be set to zero. Once that $\mathbf{W}_{0}$ and $\lambda_{\mathrm{s}}$ are given (set by default or provided by the user) the corresponding estimate of $\mathbf{z}$ is given by
	%     $$
	%     \hat{\mathbf{z}}=\left(\mathbf{W}_{0}+\lambda_{\mathrm{s}} \mathbf{D}_{\mathrm{s}}^{\mathrm{T}} \mathbf{D}_{\mathrm{s}}\right)^{-1} \mathbf{W}_{0} \mathbf{y}
	%     $$
	%     For example, an optical emission (OES) spectrum is plotted Figure 1 along with two smoothed versions shown for $\mathbf{W}_{0}=\mathbf{I}$ and $\lambda_{\mathrm{s}}=0.1$ (low smoothing) and $\lambda_{\mathrm{s}}=10$ (stronger smoothing).   
	% **Original paper states use of the first derivative**  
	% --> second derivative is very similar to SS
	% \begin{my_pros_cons_table}{
	%     \item 1
	%     \item 2
	%   }{
	%     \item 1
	%     \item 2
	%   }
	% \end{my_pros_cons_table}
