\chapter{Interpolation Methods}

\begin{my_pros_cons_table}{
        \item item 1
        \item item 2
    }{
        \item 1
        \item 2
    }
\end{my_pros_cons_table}

\section{Setting}

We are given data in the form of $\left(x_{i}, Y_{i}\right)$ for $i=1, \ldots, n)$. Assume that it can be represented by
$$
    Y_{i}=m\left(x_{i}\right)+\varepsilon_{i},
$$
where $\varepsilon_i$ is some noise and $m: \mathbb{R} \rightarrow \mathbb{R}$ being some (non-parametric regression) function. If we assume that $\varepsilon_{1}, \ldots, \varepsilon_{n}$ i.i.d. with $\mathbb{E}\left[\varepsilon_{i}\right]=0$ then $$m(x)=\mathbb{E}[Y \mid x]$$
Different assumptions on $m$ will lead to the following models:



\section{Methods - Description}

\subsection{Kernel Regression}
\label{sec:Kernel}
As described previously, we would like to estimate
\begin{equation}
    \label{eq:nadaraya}
    \mathbb{E}[Y \mid X=x]
    = \int_{\mathbb{R}} y f_{Y \mid X}(y \mid x) d y
    =\frac{\int_{\mathbb{R}} y f_{X, Y}(x, y) d y}{f_{X}(x)},
\end{equation}
where $f_{Y \mid X}, f_{X, Y}, f_{X}$ denote the conditional, joint and marginal densities.
This can be done with a kernel $K$:
$$
    \hat{f}_{X}(x)=\frac{\sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h}\right)}{n h}, \hat{f}_{X, Y}(x, y)=\frac{\sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h}\right) K\left(\frac{y-Y_{i}}{h}\right)}{n h^{2}}
$$
By plugging the above into equation \ref{eq:nadaraya} we arrive at the \textit{Nadaraya-Watson} kernel estimator:
$$\hat{m}(x)=\frac{\sum_{i=1}^{n} K\left(\left(x-x_{i}\right) / h\right) Y_{i}}{\sum_{i=1}^{n} K\left(\left(x-x_{i}\right) / h\right)}$$

**Pros**:
- can be assigned degrees of freedom (trace of the hat-matrix)
- estimation of the noise variance $\hat \sigma_\varepsilon^2$ (XXX c.f. CompStat 3.2.2)

**Cons**:
- choice of kernel
- if the $x \mapsto K(x)$ is not continuous, $\hat m $ isn't either
- choice of bandwidth, especially if $x_i$ are not equidistant.

**Examples:**
Normal, Box
For local bandwidth selection see Brockmann et al. (1993) XXX


\subsection{loess}
\label{sec:loess}


\subsection{Savitzky-Golay Filter}
\label{sec:Savitzky–Golay}
The \textit{Savitzky-Golay Filter}, introduced in \cite{savitzkySmoothingDifferentiationData1964} is a technique in signal processing and can be used to filter out high frequencies (low-pass filter) as argued in \cite{schaferWhatSavitzkyGolayFilter2011}. Furthermore, it also can be used for smoothing by filtering high frequency noise while keeping the low frequency signal.
First we choose a window size $m$. Then, for each point $j \in \{m, m+1, \dots, n-m\}$ we fit a polynomial of degree $k$ by:
$$\hat y_j=\min_{p\in P_k}\sum_{i=-m}^{m}(p (x_{j+i})-y_{i+j})^{2},$$
were $P_k$ denotes the Polynomials of degree $k$ over $\mathbb R$.

For equidistant points this can efficiently be calculated by
$$
    \hat y_{j}=\sum_{i=-m}^{m} c_{i} y_{j+i},
$$
where the $c_i$ are only dependent on the $m$ and $k$ and are tabulated in the original paper.

**Pros**
- popular technique in signal processing
- efficient calculation for equidistant points

**Cons**
- no natural way of how to estimate points which are not in the data. XXX

\subsubsection*{Interpolation and Smoothing}
In a rather famous paper \cite{chenSimpleMethodReconstructing2004a} a ``robust'' method based on the Savitzky-Golay has been used.
The method is based on the assumption that due to atmospheric effects the observed NDVI tends to be underestimated and that it cannot increase too quickly\footnote{The latter is argued by the biological impossibility of such fast vegetation changes}.

\textbf{Algorithm:}
\begin{enumerate}
    \item Remove points which are labeled as cloudy
    \item Remove points which would indicate an increase greater than 0.4 within 20 days
    \item Linearly interpolate to obtain an equidistant time series $X^0$
    \item Apply the Savitzky-Golay Filter to obtain a new time series $X^1$
    \item Update $X^1$ by applying again a Savitzky-Golay Filter but this time weight the observations by $w_i = \min\left(1, 1 - \frac{X^1_i-X^0_i}{\max_i\|X^1_i-X^0_i\|}\right)$. This reduces the impact of outliers\footnote{Here we call a point $i$ an outlier if $X^0_i<X^1_i$.} and by repeating this step the authors obtain an ``upper NDVI envelope''
\end{enumerate}

Pros
- Upper envelope matches intuition for the NDVI. Therefore, it is robust against outliers with small values.

Cons
- Not generalizable to other spectral indices.
- Linear interpolation to account for missing data might be not appropriate.
- No smooth interpolation between two measurements.

\subsubsection*{Extension: Spatial-Temporal-Savitzky-Golay Filter}
One notable adaptation of the Savitzky-Golay is the presented by \cite{caoSimpleMethodImprove2018b}. The key difference is the additional assumption of the cloud cover being discontinuous and that we can improve by looking at adjacent pixels\footnote{Here, we say that a pixel is adjacent if it is the same pixel but from a different year (keeping the same day of the year) or (if not enough of such temporal-adjacent pixel are found) it is spatially adjacent}. Because we are working with rather high resolution satellite data, and we need the variance in the predictors we will waive this extension.

\subsection{Double Logistic}
\label{sec:double_logistic}
The Double Logistic smoothing as introduced in \cite{beckImprovedMonitoringVegetation2006} heavily relies on shape assumptions of the fitted curve (i.e. the NDVI time series).

Assumptions:
\begin{itemize}
    \item There is a minimum NDVI level $Y_{\min}$ in the winter (e.g. due to evergreen plants), which might be masked by snow. This can be estimated beforehand, taking into several years into account.
    \item The growth cycle can be divided into an increase and a decrease period where the time series follows a logistic function. The maximum increase (or decrease) is observed at $t_0$ (or $t_1$) with a slope of $d_0$ (or $d_1$).
\end{itemize}

The equation of the double-logistic fit is given by:
\begin{equation*}
    Y(t) = Y_{\min} + \left(Y_{\max}-Y_{\min}\right)\left(\frac{1}{1+e^{-d_0(t-t_0)}}+\frac{1}{1+e^{-d_1(t-t_1)}}-1\right)
\end{equation*}
Where the five free parameters: $Y_{\max}$, $d_0$, $d_1$, $t_0$, $t_1$ are initially estimated by least squares. Similar as for the Savitzky-Golay Filter (c.f. section~\ref{sec:Savitzky–Golay}) we reestimate (only once) the parameters by giving less weight to the overestimated observations and more weight to the underestimated observations\footnote{For the details on the weights we refer to \cite{beckImprovedMonitoringVegetation2006}}.

\subsection{Fourier Approximation}
\label{sec:fourier_approx}
$$
    \operatorname{NDVI}(t)=\sum_{j=0}^{2} a_{j} \times \cos \left(j \times \Phi_{t}\right)+b_{j} \times \sin \left(j \times \Phi_{t}\right)
$$
where $\Phi=2 \pi \times(t-1) / n$.

\cite{beckImprovedMonitoringVegetation2006} shows in their lag-plots a heavy autocorrelation of resiudals

% \begin{my_figure}[h]{keepaspectratio}{interpol/fourier-dl-comparison}
%     \caption{Here we observe the nice fitting possibilitys of the two parametric methods but notice also some misbehaviour}
% \end{my_figure}

% \subsection{Polynomial interpolation}
% \label{sec:Polynomial}

% \subsection{Polynomial approximation}
% \label{sec:Polynomial}

\subsection{Cubic Smoothing Splines}
\label{sec:Cubic}
We interpolate with a function in $C^2$ (space of three time continuous differentiable functions) which is defined piecewise by cubic polynomials.
**Pros**
Regression splines (B-splines)
\cite{woodSmoothingParameterModel2016}
use a basis of the spline space (e.g. B-splines or j-th cardinal basis) and fit the splines of degree k to approximate the data.

\subsection{B-splines}
\label{sec:B}
from \cite{lycheSplineMethods2005}
$$
    S(x)=\sum_{j=0}^{n-1} c_{j} B_{j, k ; t}(x)
$$
$$
    \begin{array}{r}
        B_{i, 0}(x)=1, \text { if } t_{i} \leq x<t_{i+1}, \text { otherwise } 0 \\
        B_{i, k}(x)=\frac{x-t_{i}}{t_{i+k}-t_{i}} B_{i, k-1}(x)+\frac{t_{i+k+1}-x}{t_{i+k+1}-t_{i+1}} B_{i+1, k-1}(x)
    \end{array}
$$

**Smoothing:**
We can relax the constrain that we have to perfectly interpolate. Thus we use the minimum number of knots\footnote{SciPy uses FITPACK and DFITPACK, the documentation suggests that smoothness is achieved by reducing the number knots used} such that:
$\sum_{i=1}^n(w (y_i - \hat y_i))^2 \leq s$

**Pros**
- can be assigned degrees of freedom
- extendable to "smooth" version
- performs also well if points are not equidistant

**Cons**
- smoothing process does not translate well to a interpretation (unlike smoothing splines)
- choice of smoothing parameter $s$

\subsection{Natural Smoothing Splines}
\label{sec:Natural}
Let $\mathcal F$ be the Sobolev space (the space of functions of which the second derivative is integrable). Then the unique\footnote{Strictly speaking it is only unique for $\lambda > 0$} minimizer
$$\hat m :=\argmin_{f \in \mathcal F} \sum_{i=1}^{n}\left(Y_{i}-{f}\left(x_{i}\right)\right)^{2}+\lambda \int {f}^{\prime \prime}(x)^{2} d x$$
is a natural\footnote{It is called natural since it is affine outside the data range ($\forall x\notin [x_1, x_n]:\hat m''(x) = 0$)} cubic spline.

**Pros:**
- can be assigned degrees of freedom (trace of the hat-matrix)
- efficient estimation (closed form solution)
- intuitive penalty (we don't want the function to be too ``wobbly'' --- change slopes)
- performs also well if points are not equidistant
- fixes the Runge's phenomenon (fluctuation of high degree polynomial interpolation)

**Cons:**
- choose $\lambda$

\subsection{Penalized Regression Splines}
\label{sec:Penalized}
Intuition: similar as Natural Smoothing Splines, but we choose knots

\subsection{Kriging}
\label{sec:Kriging}

Kriging was developed in geostatistics to deal with autocorrelation of the response variable at nearby points. By applying the notion that two spectral indices which are (timewise) close should also take similar values we justify the application of Kriging. In the end we would like to fit a smooth Gaussian process to the data. For this subsection we will follow \cite{diggleGaussianModelsGeostatistical2007}.

\subsubsection*{Definitions and Assumptions}

A \textit{Gaussian Process} $\{S(t) : t\in \mathbb R\} $ is a stochastic process if $(S(t_1),\dots,S(t_k))$ has a multivariate Gaussian distribution for every collection of times ${t_1, \dots , t_k}$.
$S$ can be fully characterized by the mean $\mu(t):=E[S(t)]$ and its covariance function $\gamma\left(t, t^{\prime}\right)=\operatorname{Cov}\left(S(t), S\left(t^{\prime}\right)\right)$

Assumption:
We will assume the Gaussian process to be stationary. That is for $\mu(t)$ to be constant in $t$ and $\gamma(t,t')$ to depend only on $h=t-t'$. Thus, we will write in the following only $\gamma(h)$.\footnote{Note that the process is also \textit{isotropic} (i.e. $\gamma(h)=\gamma(\|h\|$) since we are in a one-dimensional setting and the covariance is symmetric.}


We also define the variogram of a Gaussian process as
$$V(h):=V\left(t, t+h\right):=\frac{1}{2} \operatorname{Var}\left(S(t)-S(t+h)\right)\\ %align XXX
    =(\gamma(0))^2(1-\operatorname{corr}(S(t),S(t+h)))
$$
And decide to use a gaussian Variogram defined by
$$V(h) = p \cdot\left(1-e^{-\frac{h^{2}}{\left(\frac{4}{7} r\right)^{2}}}\right)+n,$$
where $h$ is the distance, $n$ is the nugget, $r$ is the range and $p$ is the partial sill visuilized in figure~\ref{fig:interpol/kriging-gauss-variogram}.\footnote{Strictrly speaking we use a scaled version of the variogram. Thus only the ratio of $p/n$ matters.}
\begin{my_figure}[h]{width=0.7\textwidth}{interpol/kriging_gauss_variogram}
    \caption{Gaussian Variogram with nugget=1, partial sill=3, range=55}
    \label{fig:interpol/kriging_gauss_variogram}
\end{my_figure}

\begin{my_figure}{width=1\textwidth}{interpol/kriging_parameter}
\end{my_figure}


\subsection{Other Methods to study:}
From inroduction of \cite{chenSimpleMethodReconstructing2004a}:\\
(1) threshold-
based methods, such as the best index slope extraction
algorithm (BISE) (Viovy et al., 1992); (2) Fourier-based
fitting methods (Cihlar, 1996; Roerink et al., 2000; Sellers
et al., 1994); and (3) asymmetric function fitting methods
such as the asymmetric Gaussian function fitting approach
(Jonsson Eklundh, 2002) and the weighted least-squares
linear regression approach (Swets et al., 1999).
